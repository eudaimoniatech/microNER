{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]\n",
    "\n",
    "\n",
    "def createMatrices(sentences, label2Idx, case2Idx,char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "\n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                if x in char2Idx.keys():\n",
    "                    charIdx.append(char2Idx[x])\n",
    "                else:\n",
    "                    charIdx.append(char2Idx['UNKNOWN'])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(word)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_germeval(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            temp = [splits[1],splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproecessing data from Conll\n",
    "def get_sentences_conll(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "testSentences = get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O'], ['bis', 'O'], ['1953', 'O'], ['wurde', 'O'], ['der', 'O'], ['n√∂rdliche', 'O'], ['Teil', 'O'], ['als', 'O'], ['Jugendburg', 'O'], ['des', 'O'], ['Kolpingwerkes', 'B-OTH'], ['gebaut', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load label mapping\n",
    "import json\n",
    "indexMappings = json.load(open(\"final_model_germeval.indexes\", \"r\"))\n",
    "idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "label2Idx = indexMappings[1]\n",
    "char2Idx = indexMappings[2]\n",
    "case2Idx = indexMappings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-OTHderiv', 1: 'I-OTH', 2: 'B-ORGpart', 3: 'B-LOCderiv', 4: 'I-PER', 5: 'B-ORGderiv', 6: 'I-PERpart', 7: 'I-OTHpart', 8: 'I-ORGpart', 9: 'I-LOC', 10: 'I-ORG', 11: 'B-PERderiv', 12: 'B-LOCpart', 13: 'O', 14: 'I-LOCderiv', 15: 'I-PERderiv', 16: 'B-PERpart', 17: 'B-ORG', 18: 'I-OTHderiv', 19: 'I-ORGderiv', 20: 'I-LOCpart', 21: 'B-OTHpart', 22: 'B-OTH', 23: 'B-LOC', 24: 'B-PER'}\n",
      "{'B-OTH': 22, 'I-ORGderiv': 19, 'B-ORGderiv': 5, 'B-PERpart': 16, 'I-PERpart': 6, 'I-LOCpart': 20, 'I-OTH': 1, 'I-PERderiv': 15, 'B-OTHderiv': 0, 'I-ORGpart': 8, 'I-ORG': 10, 'O': 13, 'B-PERderiv': 11, 'B-LOCderiv': 3, 'B-ORG': 17, 'I-PER': 4, 'B-LOC': 23, 'I-OTHderiv': 18, 'B-ORGpart': 2, 'I-OTHpart': 7, 'I-LOCderiv': 14, 'B-PER': 24, 'B-OTHpart': 21, 'B-LOCpart': 12, 'I-LOC': 9}\n",
      "{'≈ì': 271, '—î': 281, '‚ñ™': 292, '·ªá': 305, '—Ä': 25, '√ó': 14, '√¶': 273, '√â': 72, '\\x9a': 93, 'o': 222, '¬©': 13, 'K': 303, 'i': 39, '¬ª': 158, 'Œ¨': 133, 'D': 141, 'Î£®': 285, '‚Ä∫': 314, '—é': 113, '√°': 274, 'Œµ': 316, '\\x95': 263, 'd': 0, 'ƒá': 279, 'B': 244, 'ƒ∞': 59, '–Ω': 20, '‚â§': 21, '–æ': 163, '[': 49, '√´': 146, '≈Ü': 313, '≈ª': 151, ']': 149, '4': 220, '3': 160, '≈Å': 109, '&': 194, '‚Äù': 207, '¬∏': 104, '‚àí': 268, 'P': 64, 'È∑π': 22, 'ÿß': 250, 'k': 102, '≈ë': 179, '√ß': 201, 't': 53, 'ÿ∂': 226, '*': 167, 'Âçö': 223, '¬≥': 215, 'Â£´': 153, '‚Ç¨': 15, '$': 288, 'q': 41, '√Å': 88, 'R': 171, '.': 116, '¬µ': 231, 'Œ∑': 43, '√ü': 301, 'Œõ': 5, '¬π': 317, 'Q': 152, 'Êùé': 26, '≈ç': 189, '‚Äô': 96, '—Ç': 144, 'h': 234, 'U': 205, 'ƒÅ': 283, 'n': 74, '0': 294, 'ÂÆà': 315, 'X': 131, '—É': 308, '_': 296, '≈æ': 1, '‚Äö': 325, '‰πù': 202, '√º': 137, 'r': 219, '·∏´': 8, 'Âçó': 203, '—è': 19, 'ÂÖ¨': 9, '(': 40, 'T': 204, '≈º': 73, '≈Ç': 63, '—ó': 55, '‚Äê': 100, '1': 326, 'O': 37, 'œÇ': 54, 'M': 240, '–£': 127, 'œà': 300, '√ñ': 176, 'g': 90, '√¢': 50, '–¥': 196, 'f': 46, 'ƒÖ': 323, '≈õ': 114, 'N': 150, '‚Äî': 123, 'p': 309, 'v': 224, 'C': 148, 'G': 235, '¬∑': 126, 'Œª': 70, '@': 173, '¬∞': 125, '√†': 297, 'I': 321, '¬≤': 182, '√é': 286, ' ª': 128, '%': 81, '√Ü': 232, '≈†': 236, 'W': 199, '–∂': 122, 'Ìïô': 60, '√Æ': 156, 'Œ≥': 71, 'Ê®ì': 192, '√∫': 280, 'x': 197, '–∏': 110, '‚Äú': 10, \"'\": 304, '—Å': 38, 'Á´†': 30, '‚Äì': 86, '¬±': 311, 'Œø': 138, '}': 16, 'Êù±': 168, 'ÂÜ≤': 147, 'È∂¥': 191, '>': 92, '‚Üí': 99, 'Œ∫': 287, 'ÁÆó': 249, '-': 134, '—Ö': 79, 'J': 239, 'Ë°ì': 290, '–°': 256, '√•': 265, 'ƒü': 221, 'c': 299, '–ï': 143, 'Z': 245, '—ã': 183, 'ŸÄ': 29, 'œÖ': 28, '?': 259, '‚âò': 312, 'ÎåÄ': 190, '~': 140, '¬£': 124, '–¢': 35, 'y': 165, '≈ü': 266, 'ÊüØ': 243, 'Œπ': 12, '5': 246, 'a': 198, '√ª': 184, '√∂': 211, '–±': 237, '‚Äπ': 89, 'ŒΩ': 66, 'ƒ©': 277, '√™': 275, '<': 107, 'Ÿä': 242, '–≥': 262, '„É©': 276, '√£': 51, 'ÂØù': 209, '·∏≥': 112, 'ƒã': 166, '√à': 36, 'ƒ´': 248, 'Îèô': 4, 'Œ±': 255, 'l': 267, '√≥': 174, 'Œ≠': 291, '¬´': 178, '√∏': 85, '≈•': 233, 'V': 161, '‚Ä≥': 195, 'Y': 200, 'Œ†': 80, '–õ': 61, '‚Äû': 33, 'ÿ®': 56, 'F': 253, 'ÊÆø': 130, '\\xad': 34, '\"': 319, '‚Äò': 135, '√Ä': 264, '2': 247, '‚Ä†': 17, 'Â¶É': 27, '\\x80': 188, '—å': 169, '7': 228, 'ƒ±': 119, '√§': 118, 'ƒå': 65, '≈ô': 78, 'Œ≤': 106, 'œå': 306, 'ÈÄ†': 44, 'A': 69, ';': 57, '–ª': 11, 'e': 23, 'ƒõ': 108, '«í': 103, 'ƒô': 3, '!': 269, 'UNKNOWN': 328, '√ú': 193, '–∞': 181, '\\x94': 258, '√∞': 155, '–¶': 98, 'u': 257, '≈û': 142, '6': 213, 'œÅ': 186, '9': 282, 'œÉ': 254, 'ƒ†': 67, '8': 302, 'Ë≤¥': 208, ',': 91, '≈´': 68, '√ò': 48, '√û': 7, '√≠': 105, '≈í': 175, '√®': 284, 'j': 212, 'Î≥Ñ': 101, 'ƒß': 217, '√Ö': 6, 'œÑ': 132, '–®': 272, 'œÜ': 252, '`': 97, '≈è': 251, '„Ç™': 278, '=': 289, '–ø': 83, '·∫ø': 216, '≈°': 111, '\\u200e': 318, 'Âè∞': 139, '—ä': 58, '+': 185, 'Âà•': 31, '¬Ω': 94, '√¥': 261, 'ŸÜ': 76, '‚Ä¶': 229, 'w': 115, '–í': 47, 'ƒç': 162, '/': 218, 'ÂÇ≥': 120, '–∑': 238, '–∫': 206, '#': 225, '–≤': 24, '¬§': 121, '≈Ω': 172, '‚ãÖ': 320, 'œÄ': 159, '¬¥': 298, '…®': 177, '\\x92': 270, '≈∫': 145, '‰Ωê': 42, '\\x99': 307, 'ƒÉ': 214, 'L': 95, '≈à': 77, '\\x96': 87, 'z': 18, 'b': 170, '·ø¶': 117, ':': 241, '√Ç': 45, 'S': 310, '–º': 84, '√Ñ': 82, '√±': 62, 'ƒì': 295, 'H': 227, '√©': 157, '¬ß': 136, 's': 75, 'ÿ£': 129, ')': 32, '√µ': 322, 'Â§™': 230, '≈Ñ': 52, 'Êáø': 164, '–ò': 187, 'm': 180, '‚äÉ': 2, '–µ': 210, '–ü': 154, '–π': 324, '√Ω': 293, '–ú': 260, 'E': 327}\n",
      "{'allLower': 1, 'numeric': 0, 'mainly_numeric': 5, 'PADDING_TOKEN': 7, 'contains_digit': 6, 'other': 4, 'initialUpper': 3, 'allUpper': 2}\n"
     ]
    }
   ],
   "source": [
    "print(idx2Label)\n",
    "print(label2Idx)\n",
    "print(char2Idx)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "test_batches = createBatches(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch            \n",
    "                sentence = batch[index]\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                for word in sentence:\n",
    "                    word, label = word\n",
    "                    casing =getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    # word_vector = ft.get_word_vector(word.lower())\n",
    "                    word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                temp_output = to_categorical(temp_output, 25)\n",
    "                output_labels.append(temp_output)\n",
    "            yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == len(batch):\n",
    "            break\n",
    "    return(true_labels, pred_labels)\n",
    "\n",
    "tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,52,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    kernel_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in kernel_sizes:\n",
    "        conv = TimeDistributed(Conv1D(\n",
    "                             kernel_size=sz,\n",
    "                             filters=32,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1))(embed_char_out)\n",
    "        conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "        conv = TimeDistributed(Flatten())(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    output = concatenate([words_input, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10528       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 1, 32)  0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 1, 32)  0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 1, 32)  0           time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 32)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 32)     0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 32)     0           time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 400)    968000      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 25)     10025       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 25)     1325        time_distributed_10[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,002,326\n",
      "Trainable params: 1,002,262\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7745224991906766, 0.8343504795117699, 0.8033240997229917)\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "print(compute_f1(true_labels, pred_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
