{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "# from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.utils import Sequence\n",
    "from numpy import newaxis\n",
    "from random import shuffle\n",
    "import math\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):\n",
    "    \n",
    "    if word == 'PADDING_TOKEN':\n",
    "        return(caseLookup['PADDING_TOKEN'])\n",
    "    \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "   \n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_wiki(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence[:max_sequence_length])\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            if len(splits) != 3:\n",
    "                # print(line)\n",
    "                continue\n",
    "            temp = [splits[1],splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mapping\n",
    "import json\n",
    "indexMappings = json.load(open(\"../models/final_model_germeval.indexes\", \"r\"))\n",
    "idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "label2Idx = indexMappings[1]\n",
    "char2Idx = indexMappings[2]\n",
    "case2Idx = indexMappings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4c717ebc4b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_sequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainSentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentences_wiki\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/weakly_supervised_ner_wiki.train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdevSentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentences_wiki\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/weakly_supervised_ner_wiki.dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-a338e59ceff5>\u001b[0m in \u001b[0;36mget_sentences_wiki\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kerasenv/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences_wiki('../data/weakly_supervised_ner_wiki.train')\n",
    "devSentences = get_sentences_wiki('../data/weakly_supervised_ner_wiki.dev')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, s in enumerate(trainSentences):\n",
    "    for t in s:\n",
    "        if len(s) > max_sequence_length:\n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471498\n",
      "1400000\n",
      "71498\n"
     ]
    }
   ],
   "source": [
    "# POS ALTERNATIVE\n",
    "\n",
    "trainSentences = get_sentences_wiki('../data/weakly_supervised_pos_wiki.train')\n",
    "print(len(trainSentences))\n",
    "devSentences = trainSentences[1400000:]\n",
    "trainSentences = trainSentences[:1400000]\n",
    "\n",
    "#devSentences = trainSentences[1000:1200]\n",
    "#trainSentences = trainSentences[:1000]\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "{0: 'PADDING_TOKEN', 1: 'PDAT', 2: '$(', 3: 'APPR', 4: 'VAINF', 5: 'PWAV', 6: 'PWAT', 7: 'VMPP', 8: 'PWS', 9: 'NNE', 10: 'VVIMP', 11: 'FM', 12: 'APZR', 13: 'KON', 14: 'PDS', 15: 'PIAT', 16: 'TRUNC', 17: 'VVIZU', 18: 'PIS', 19: '$.', 20: 'VAPP', 21: 'ART', 22: 'PTKANT', 23: 'VVINF', 24: 'NN', 25: 'PTKNEG', 26: 'PPOSS', 27: 'NE', 28: 'PTKVZ', 29: 'VAIMP', 30: 'VVFIN', 31: 'PRF', 32: 'KOKOM', 33: 'PRELAT', 34: 'ADV', 35: 'XY', 36: 'APPO', 37: 'PPER', 38: 'KOUS', 39: 'VAFIN', 40: 'CARD', 41: 'KOUI', 42: 'VMINF', 43: 'ADJA', 44: 'ITJ', 45: 'VVPP', 46: 'PROAV', 47: 'VMFIN', 48: '$,', 49: 'ADJD', 50: 'PTKA', 51: 'PRELS', 52: 'APPRART', 53: 'PPOSAT', 54: 'PTKZU'}\n"
     ]
    }
   ],
   "source": [
    "labelSet = set()\n",
    "for dataset in [trainSentences, devSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label in sentence:\n",
    "            labelSet.add(label)\n",
    "\n",
    "label2Idx = {\"PADDING_TOKEN\":0}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "print(len(label2Idx))\n",
    "print(idx2Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trainSentences[1096633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['K-525', 'NN'], ['Archangelsk', 'NE'], ['K-525', 'NE'], ['wurde', 'VAFIN'], ['am', 'APPRART'], ['25', 'ADJA'], ['.', '$('], ['Juli', 'NN'], ['1975', 'CARD'], ['auf', 'APPR'], ['Kiel', 'NE'], ['gelegt', 'VVPP'], ['.', '$.']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'contains_digit': 7, 'PADDING_TOKEN': 0, 'numeric': 1, 'initialUpper': 4, 'allUpper': 3, 'allLower': 2, 'other': 5, 'mainly_numeric': 6}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'а': 299, '傳': 2, 'X': 9, 'O': 11, 'Е': 258, 'ū': 98, 'ę': 99, 'ö': 116, 'Œ': 78, 'N': 157, 'β': 247, '妃': 149, '„': 233, '‚': 146, 'æ': 124, '[': 54, 'ɨ': 10, '佐': 232, 'ő': 112, 'π': 148, '→': 323, 'ī': 227, 'V': 319, '=': 110, 'ψ': 128, '*': 52, '<': 123, '\\x9a': 61, 'G': 256, 'ú': 53, 'É': 143, 'ø': 314, '§': 283, 'ł': 70, 'ض': 210, '$': 43, 'ラ': 310, '\\x80': 23, 'â': 100, 'B': 33, 'Т': 260, 'p': 46, 'р': 21, 'o': 79, 'à': 291, '_': 29, '?': 178, '>': 94, '대': 63, 'ã': 167, '太': 264, '殿': 102, 'Ü': 203, '0': 269, 'ḳ': 159, 'Å': 104, 'ë': 280, 'ž': 274, 'Y': 37, '}': 45, '別': 156, 'I': 130, 'č': 152, '¹': 281, '公': 31, 'Ä': 140, ')': 229, 'Š': 235, 'ν': 158, 'ь': 7, 'С': 253, 'q': 321, '-': 244, 'φ': 292, '冲': 27, '−': 164, 'î': 114, '루': 306, 'ğ': 76, 'т': 238, 'W': 301, '東': 286, 'r': 320, 'ʻ': 162, 'ç': 251, 'é': 313, 'm': 139, 'В': 295, 'ô': 191, 'ć': 16, '£': 243, '鷹': 60, '동': 107, 'х': 198, '‘': 145, 'σ': 257, 'd': 40, 'П': 300, '@': 207, 'ـ': 135, '·': 115, 'у': 4, ',': 200, 'Ġ': 106, 'Ż': 142, 'ō': 250, 'Á': 218, 'g': 127, '‹': 242, 'È': 49, '+': 316, '`': 113, 'ź': 304, '³': 296, '“': 212, 't': 174, '×': 239, 'ο': 71, '/': 138, 'Ş': 166, 'в': 19, '€': 5, 'i': 277, '°': 226, 'k': 201, 'с': 322, 'õ': 288, 'ħ': 272, 'л': 184, 'Ш': 161, '—': 328, 'ń': 136, 'Ž': 205, 'ά': 111, 'Æ': 26, '’': 118, 'c': 125, 'أ': 202, 'ä': 267, 'ā': 186, 'M': 298, 'å': 132, 'п': 170, 's': 169, 'Ø': 67, 'ж': 268, 'u': 228, 'ņ': 325, '\\x94': 13, 'ы': 137, '±': 234, 'ş': 1, 'z': 73, 'K': 88, ']': 57, 'ε': 261, 'з': 282, '»': 196, 'ż': 236, '©': 179, 'ı': 187, '▪': 290, '\\x95': 297, 'H': 48, 'F': 175, '九': 303, 'İ': 42, 'ρ': 121, '4': 89, 'À': 289, '\\x99': 188, 'ї': 204, 'ē': 194, '.': 276, '柯': 82, 'б': 8, 'ệ': 307, '†': 55, 'L': 279, '«': 275, 'A': 50, 'E': 312, '8': 86, 'ř': 85, 'ð': 285, '\"': 144, 'x': 223, 'ê': 93, '–': 15, 'µ': 294, '\\x96': 119, 'ب': 284, '&': 308, '≘': 147, 'è': 176, '학': 150, 'ĩ': 173, 'Ц': 255, '李': 22, '6': 120, 'Ö': 237, '별': 17, 'У': 230, 'Č': 68, 'j': 293, 'ι': 109, '²': 91, 'Λ': 44, 'J': 129, '…': 326, ';': 318, 'P': 74, 'PADDING_TOKEN': 0, 'ю': 81, '南': 248, '⊃': 24, 'ῦ': 305, 'ą': 153, 'h': 3, 'í': 254, 'ς': 97, 'Q': 263, 'T': 224, 'ي': 181, '鶴': 262, '›': 96, '⋅': 215, 'l': 278, 'д': 287, 'a': 225, 'я': 193, '”': 28, 'n': 59, 'ó': 246, \"'\": 87, '術': 38, '章': 265, 'e': 108, 'f': 163, '\\xad': 172, '9': 84, 'ñ': 141, 'w': 56, 'λ': 69, 'オ': 208, '¸': 171, 'S': 259, '1': 315, '算': 185, 'Π': 36, 'й': 220, 'U': 206, ':': 199, '2': 134, 'ċ': 12, 'к': 302, 'UNKNOWN': 329, 'ť': 105, 'ü': 35, '¤': 273, 'Î': 189, 'М': 65, 'η': 14, '‐': 90, '造': 182, '博': 192, 'ъ': 101, '守': 221, '#': 209, 'R': 311, 'о': 249, '貴': 160, 'v': 39, '5': 266, 'š': 75, 'г': 92, 'υ': 6, 'є': 222, 'Л': 25, 'м': 217, 'ň': 165, '士': 151, 'D': 327, 'ý': 83, '7': 240, '~': 324, 'ن': 241, '!': 183, 'y': 213, 'α': 77, '≤': 177, 'Â': 72, 'û': 20, 'ό': 117, '樓': 155, '寝': 131, 'ḫ': 64, '\\x92': 219, 'и': 197, 'е': 30, '″': 126, 'И': 47, '台': 216, 'ă': 309, 'τ': 133, 'C': 270, '´': 103, 'ß': 214, '\\u200e': 18, 'ě': 317, '½': 51, '%': 252, 'ŏ': 95, 'Z': 245, 'á': 34, 'Ł': 122, '懿': 154, 'ế': 231, 'γ': 62, 'œ': 180, 'ǒ': 271, 'b': 195, 'н': 80, 'ś': 58, 'κ': 211, 'Þ': 190, 'ا': 32, '3': 168, 'έ': 66, '(': 41}\n"
     ]
    }
   ],
   "source": [
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Die', 'ART'], ['Kerr-Newman-Metrik', 'NN'], ['ist', 'VAFIN'], ['eine', 'ART'], ['exakte', 'ADJA'], ['Lösung', 'NN'], ['für', 'APPR'], ['sowohl', 'KON'], ['rotierende', 'ADJA'], ['als', 'KON'], ['auch', 'ADV'], ['elektrisch', 'ADJD'], ['geladene', 'ADJA'], ['schwarze', 'ADJA'], ['Löcher', 'NN'], ['in', 'APPR'], ['vier', 'CARD'], ['Dimensionen', 'NN'], ['.', '$.']]\n"
     ]
    }
   ],
   "source": [
    "print(devSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "# ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_sequences(sentences):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    all_true_labels = []\n",
    "    for s in sentences:\n",
    "        all_true_labels.append([w[1] for w in s])\n",
    "    all_pred_labels = model.predict_generator(NerSequence(sentences))\n",
    "    \n",
    "    for s_id, s in enumerate(all_true_labels):\n",
    "        not_padded_true = []\n",
    "        not_padded_pred = []\n",
    "        predicted_labels = get_label_from_categorical(all_pred_labels[s_id])\n",
    "        for t_id, t in enumerate(s):\n",
    "            if t != 'PADDING_TOKEN': # skip PADDING_TOKEN \n",
    "                not_padded_true.append(label2Idx[t])\n",
    "                not_padded_pred.append(predicted_labels[t_id])\n",
    "        true_labels.append(not_padded_true)\n",
    "        pred_labels.append(not_padded_pred)\n",
    "    \n",
    "    return(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerSequence(Sequence):\n",
    "    def __init__(self, sentence_data, shuffle_data = False, batch_size=32):\n",
    "        self.sentence_data = sentence_data\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.sentence_data))\n",
    "        if self.shuffle_data:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.sentence_data) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_x, batch_y = self.get_processed_data(inds)\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle_data:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "    def get_processed_data(self, inds):\n",
    "        word_embeddings = []\n",
    "        case_embeddings = []\n",
    "        char_embeddings = []\n",
    "        \n",
    "        output_labels = []\n",
    "\n",
    "        for index in inds: \n",
    "            sentence = self.sentence_data[index]\n",
    "\n",
    "            temp_word= []\n",
    "            temp_casing = []\n",
    "            temp_char= []\n",
    "\n",
    "            temp_output=[]\n",
    "\n",
    "            # padding\n",
    "            words_to_pad = max_sequence_length - len(sentence)\n",
    "            for i in range(words_to_pad):\n",
    "                sentence.append(['PADDING_TOKEN', 'PADDING_TOKEN'])\n",
    "\n",
    "            # create data input for words\n",
    "            for word in sentence:\n",
    "                word, label = word\n",
    "                temp_output.append(label2Idx[label])\n",
    "\n",
    "                casing = getCasing(word, case2Idx)\n",
    "                temp_casing.append(casing)\n",
    "\n",
    "                if word == 'PADDING_TOKEN':\n",
    "                    temp_char2=np.array([char2Idx['PADDING_TOKEN']])\n",
    "                    temp_char.append(temp_char2)\n",
    "                    word_vector = [0] * nb_embedding_dims\n",
    "                    temp_word.append(word_vector)\n",
    "                else:\n",
    "                    # char\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN'])\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "\n",
    "                    # word\n",
    "                    word_vector = ft.get_word_vector(word.lower())\n",
    "                    # word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "\n",
    "            temp_char = pad_sequences(temp_char, 52)\n",
    "            word_embeddings.append(temp_word)\n",
    "            case_embeddings.append(temp_casing)\n",
    "            char_embeddings.append(temp_char)\n",
    "            temp_output = to_categorical(temp_output, len(label2Idx))\n",
    "            output_labels.append(temp_output)\n",
    "\n",
    "        return([np.array(word_embeddings), \n",
    "                np.array(case_embeddings), \n",
    "                np.array(char_embeddings)], \n",
    "               np.array(output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model_3cnn():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,52,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    kernel_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in kernel_sizes:\n",
    "        conv = TimeDistributed(Conv1D(\n",
    "                             kernel_size=sz,\n",
    "                             filters=32,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1,\n",
    "                             name='charcnn_' + str(sz)))(embed_char_out)\n",
    "        conv = TimeDistributed(MaxPooling1D(52, name = 'charcnn_maxpool'))(conv)\n",
    "        conv = TimeDistributed(Flatten())(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    output = concatenate([words_input, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5, name='token_lstm'))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx), name = 'token_dense'))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "nb_char_embedding_dims = 52\n",
    "def get_model_lstm():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,nb_char_embedding_dims,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    char_lstm = TimeDistributed(Bidirectional(LSTM(50)))(embed_char_out)\n",
    "    output = concatenate([words_input, casing, char_lstm])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "class F1History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.f1_scores = []\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.acc.append(logs.get('val_acc'))\n",
    "        true_labels, pred_labels = predict_sequences(devSentences)\n",
    "        pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "        self.f1_scores.append(f1)\n",
    "        if epoch > -1 and f1 > self.max_f1:\n",
    "            print(\"\\nNew maximum F1 score: \" + str(f1) + \" (before: \" + str(self.max_f1) + \") Saving to \" + tmp_model_filename)\n",
    "            self.max_f1 = f1\n",
    "            model.save(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, None, 1, 32)  0           time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (None, None, 1, 32)  0           time_distributed_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, None, 1, 32)  0           time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, None, 32)     0           time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, None, 32)     0           time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, None, 32)     0           time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_33[0][0]        \n",
      "                                                                 time_distributed_36[0][0]        \n",
      "                                                                 time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, None, 400)    968000      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_40 (TimeDistri (None, None, 55)     22055       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_4 (CRF)                     (None, None, 55)     6215        time_distributed_40[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,019,278\n",
      "Trainable params: 1,019,214\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -0.1225 - acc: 0.9863\n",
      "Epoch 00001: val_acc improved from -inf to 0.99009, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3651s 334ms/step - loss: -0.1225 - acc: 0.9863 - val_loss: -0.3277 - val_acc: 0.9901\n",
      "Epoch 2/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -0.5204 - acc: 0.9893\n",
      "Epoch 00002: val_acc improved from 0.99009 to 0.99060, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3553s 325ms/step - loss: -0.5204 - acc: 0.9893 - val_loss: -0.7198 - val_acc: 0.9906\n",
      "Epoch 3/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -0.9119 - acc: 0.9896\n",
      "Epoch 00003: val_acc improved from 0.99060 to 0.99078, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3533s 323ms/step - loss: -0.9119 - acc: 0.9896 - val_loss: -1.1106 - val_acc: 0.9908\n",
      "Epoch 4/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -1.3027 - acc: 0.9898\n",
      "Epoch 00004: val_acc improved from 0.99078 to 0.99080, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3560s 325ms/step - loss: -1.3028 - acc: 0.9898 - val_loss: -1.5014 - val_acc: 0.9908\n",
      "Epoch 5/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -1.6934 - acc: 0.9898\n",
      "Epoch 00005: val_acc improved from 0.99080 to 0.99094, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3723s 340ms/step - loss: -1.6934 - acc: 0.9898 - val_loss: -1.8919 - val_acc: 0.9909\n",
      "Epoch 6/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -2.0840 - acc: 0.9899\n",
      "Epoch 00006: val_acc improved from 0.99094 to 0.99095, saving model to wiki_POS_pretrained_LSTM-BiLSTM.h5\n",
      "10938/10938 [==============================] - 3698s 338ms/step - loss: -2.0840 - acc: 0.9899 - val_loss: -2.2826 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -2.4738 - acc: 0.9897\n",
      "Epoch 00007: val_acc did not improve\n",
      "10938/10938 [==============================] - 3701s 338ms/step - loss: -2.4739 - acc: 0.9897 - val_loss: -2.6725 - val_acc: 0.9909\n",
      "Epoch 8/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -2.8611 - acc: 0.9888\n",
      "Epoch 00008: val_acc did not improve\n",
      "10938/10938 [==============================] - 3559s 325ms/step - loss: -2.8611 - acc: 0.9888 - val_loss: -3.0612 - val_acc: 0.9903\n",
      "Epoch 9/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -3.2461 - acc: 0.9876\n",
      "Epoch 00009: val_acc did not improve\n",
      "10938/10938 [==============================] - 3471s 317ms/step - loss: -3.2461 - acc: 0.9876 - val_loss: -3.4459 - val_acc: 0.9887\n",
      "Epoch 10/10\n",
      "10937/10938 [============================>.] - ETA: 0s - loss: -3.6342 - acc: 0.9868\n",
      "Epoch 00010: val_acc did not improve\n",
      "10938/10938 [==============================] - 3489s 319ms/step - loss: -3.6342 - acc: 0.9868 - val_loss: -3.8395 - val_acc: 0.9895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efc5020d9e8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_filename = 'wiki_POS_pretrained_LSTM-BiLSTM.h5'\n",
    "checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = F1History()\n",
    "model = get_model_3cnn()\n",
    "model.fit_generator(\n",
    "    NerSequence(trainSentences, shuffle_data=True, batch_size=128), \n",
    "    validation_data = NerSequence(devSentences, batch_size=256), \n",
    "    epochs = 10, callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'F1History' object has no attribute 'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e316d1d0de25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'F1History' object has no attribute 'acc'"
     ]
    }
   ],
   "source": [
    "print(history.acc)\n",
    "print(history.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('wiki_POS_pretrained_LSTM-BiLSTM_10epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers.pop()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexMappings = json.load(open(\"../models/final_model_germeval.indexes\", \"r\"))\n",
    "idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "label2Idx = indexMappings[1]\n",
    "char2Idx = indexMappings[2]\n",
    "case2Idx = indexMappings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, None, 1, 32)  0           time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (None, None, 1, 32)  0           time_distributed_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_38 (TimeDistri (None, None, 1, 32)  0           time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, None, 32)     0           time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, None, 32)     0           time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_39 (TimeDistri (None, None, 32)     0           time_distributed_38[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_33[0][0]        \n",
      "                                                                 time_distributed_36[0][0]        \n",
      "                                                                 time_distributed_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, None, 400)    968000      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_40 (TimeDistri (None, None, 55)     22055       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_4 (CRF)                     (None, None, 55)     6215        time_distributed_40[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,019,278\n",
      "Trainable params: 1,019,214\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10560       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_43 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_46 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_49 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_44 (TimeDistri (None, None, 1, 32)  0           time_distributed_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_47 (TimeDistri (None, None, 1, 32)  0           time_distributed_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_50 (TimeDistri (None, None, 1, 32)  0           time_distributed_49[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_45 (TimeDistri (None, None, 32)     0           time_distributed_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_48 (TimeDistri (None, None, 32)     0           time_distributed_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_51 (TimeDistri (None, None, 32)     0           time_distributed_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, None, 404)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_45[0][0]        \n",
      "                                                                 time_distributed_48[0][0]        \n",
      "                                                                 time_distributed_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, None, 400)    968000      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_52 (TimeDistri (None, None, 26)     10426       bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_6 (CRF)                     (None, None, 26)     1430        time_distributed_52[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,002,864\n",
      "Trainable params: 1,002,800\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = get_model_3cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_input\n",
      "char_embedding\n",
      "time_distributed_31\n",
      "time_distributed_34\n",
      "time_distributed_37\n",
      "casing_input\n",
      "time_distributed_32\n",
      "time_distributed_35\n",
      "time_distributed_38\n",
      "words_input\n",
      "case_embed\n",
      "time_distributed_33\n",
      "time_distributed_36\n",
      "time_distributed_39\n",
      "concatenate_4\n",
      "bidirectional_4\n",
      "time_distributed_40\n",
      "crf_4\n",
      "---\n",
      "char_input\n",
      "char_embedding\n",
      "time_distributed_43\n",
      "time_distributed_46\n",
      "time_distributed_49\n",
      "casing_input\n",
      "time_distributed_44\n",
      "time_distributed_47\n",
      "time_distributed_50\n",
      "words_input\n",
      "case_embed\n",
      "time_distributed_45\n",
      "time_distributed_48\n",
      "time_distributed_51\n",
      "concatenate_6\n",
      "bidirectional_7\n",
      "time_distributed_52\n",
      "crf_6\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    print(l.name)\n",
    "print('---')\n",
    "for l in model2.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('wiki_POS_pretrained_LSTM-BiLSTM.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
