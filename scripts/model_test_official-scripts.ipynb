{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import fastText\n",
    "\n",
    "from validation import compute_f1\n",
    "import conlleval\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "import models\n",
    "import utils\n",
    "\n",
    "import shutil, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "def create_custom_objects():\n",
    "    instanceHolder = {\"instance\": None}\n",
    "    class ClassWrapper(CRF):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            instanceHolder[\"instance\"] = self\n",
    "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "    def loss(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"loss_function\")\n",
    "        return method(*args)\n",
    "    def accuracy(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"accuracy\")\n",
    "        return method(*args)\n",
    "    return {\"ClassWrapper\": ClassWrapper ,\"CRF\": ClassWrapper, \"loss\": loss, \"accuracy\":accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "testSentences = utils.get_sentences_germeval('../../Resources/GermEVAL/NER-de-test.tsv')\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.ft = fastText.load_model(\"../../fastText/wiki.de.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model_indexes(indexes_file):\n",
    "    indexMappings = json.load(open(indexes_file, \"r\"))\n",
    "    models.idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "    models.label2Idx = indexMappings[1]\n",
    "    models.char2Idx = indexMappings[2]\n",
    "    models.case2Idx = indexMappings[3]\n",
    "    models.max_sequence_length = 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_indexes('model_lstm_germeval_v2.0.h5.indexes')\n",
    "finalmodel = load_model('model_lstm_germeval_v2.0.h5', custom_objects=create_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8325358851674641, 0.8167691809647135, 0.8245771713375276)\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels = utils.predict_sequences(finalmodel, testSentences)\n",
    "print(compute_f1(pred_labels, true_labels, models.idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official script eval CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 101599 tokens with 6178 phrases; found: 6063 phrases; correct: 5047.\n",
      "accuracy:  81.27%; (non-O)\n",
      "accuracy:  97.73%; precision:  83.24%; recall:  81.69%; FB1:  82.46\n",
      "              LOC: precision:  85.47%; recall:  89.98%; FB1:  87.66  1796\n",
      "         LOCderiv: precision:  86.61%; recall:  93.40%; FB1:  89.88  605\n",
      "          LOCpart: precision:  76.62%; recall:  54.13%; FB1:  63.44  77\n",
      "              ORG: precision:  77.52%; recall:  75.57%; FB1:  76.53  1121\n",
      "         ORGderiv: precision:  50.00%; recall:  12.50%; FB1:  20.00  2\n",
      "          ORGpart: precision:  67.04%; recall:  69.77%; FB1:  68.38  179\n",
      "              OTH: precision:  72.20%; recall:  57.39%; FB1:  63.95  554\n",
      "         OTHderiv: precision:  64.86%; recall:  61.54%; FB1:  63.16  37\n",
      "          OTHpart: precision:  64.29%; recall:  21.43%; FB1:  32.14  14\n",
      "              PER: precision:  90.64%; recall:  91.03%; FB1:  90.84  1646\n",
      "         PERderiv: precision:  37.50%; recall:  27.27%; FB1:  31.58  8\n",
      "          PERpart: precision:  45.83%; recall:  25.00%; FB1:  32.35  24\n"
     ]
    }
   ],
   "source": [
    "import conlleval\n",
    "\n",
    "eval_file = 'test_pl.tsv'\n",
    "write_conll_file(true_labels, pred_labels, testSentences, models.idx2Label, eval_file) \n",
    "p,r,f = evaluate_conll_file(eval_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.24261916542966\n",
      "81.693104564584\n",
      "82.46058328567928\n"
     ]
    }
   ],
   "source": [
    "print(p)\n",
    "print(r)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model model_lstm_germeval_v2.0.h5\n",
      "processed 101599 tokens with 6178 phrases; found: 6063 phrases; correct: 5047.\n",
      "accuracy:  81.27%; (non-O)\n",
      "accuracy:  97.73%; precision:  83.24%; recall:  81.69%; FB1:  82.46\n",
      "              LOC: precision:  85.47%; recall:  89.98%; FB1:  87.66  1796\n",
      "         LOCderiv: precision:  86.61%; recall:  93.40%; FB1:  89.88  605\n",
      "          LOCpart: precision:  76.62%; recall:  54.13%; FB1:  63.44  77\n",
      "              ORG: precision:  77.52%; recall:  75.57%; FB1:  76.53  1121\n",
      "         ORGderiv: precision:  50.00%; recall:  12.50%; FB1:  20.00  2\n",
      "          ORGpart: precision:  67.04%; recall:  69.77%; FB1:  68.38  179\n",
      "              OTH: precision:  72.20%; recall:  57.39%; FB1:  63.95  554\n",
      "         OTHderiv: precision:  64.86%; recall:  61.54%; FB1:  63.16  37\n",
      "          OTHpart: precision:  64.29%; recall:  21.43%; FB1:  32.14  14\n",
      "              PER: precision:  90.64%; recall:  91.03%; FB1:  90.84  1646\n",
      "         PERderiv: precision:  37.50%; recall:  27.27%; FB1:  31.58  8\n",
      "          PERpart: precision:  45.83%; recall:  25.00%; FB1:  32.35  24\n",
      "Evaluating model model_lstm_germeval_v2.1.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-dc19e1c08e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mconlleval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2Label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconlleval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_conll_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mall_f1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/microNER/scripts/conlleval.py\u001b[0m in \u001b[0;36mevaluate_conll_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mtrue_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mpred_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/microNER/scripts/conlleval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(true_seqs, pred_seqs, verbose)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     (correct_chunks, true_chunks, pred_chunks,\n\u001b[0;32m--> 210\u001b[0;31m         correct_counts, true_counts, pred_counts) = count_chunks(true_seqs, pred_seqs)\n\u001b[0m\u001b[1;32m    211\u001b[0m     result = get_result(correct_chunks, true_chunks, pred_chunks,\n\u001b[1;32m    212\u001b[0m         correct_counts, true_counts, pred_counts, verbose=verbose)\n",
      "\u001b[0;32m/srv/microNER/scripts/conlleval.py\u001b[0m in \u001b[0;36mcount_chunks\u001b[0;34m(true_seqs, pred_seqs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorrect_chunk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "file_base_name = 'model_lstm_germeval_v2.'\n",
    "eval_file = 'test_pl.tsv'\n",
    "all_f1_scores = []\n",
    "\n",
    "for run_i in range(10):\n",
    "    model_file = file_base_name + str(run_i) + '.h5'\n",
    "    index_file = file_base_name + str(run_i) + '.h5.indexes'\n",
    "    if not os.path.isfile(model_file):\n",
    "        print(model_file + ' not found')\n",
    "        continue\n",
    "    if not os.path.isfile(index_file):\n",
    "        print(model_file + ' not found')\n",
    "        continue\n",
    "    \n",
    "    print('Evaluating model ' + model_file)\n",
    "    \n",
    "    load_model_indexes(index_file)\n",
    "    finalmodel = load_model(model_file, custom_objects=create_custom_objects())\n",
    "    \n",
    "    true_labels, pred_labels = utils.predict_sequences(finalmodel, testSentences)\n",
    "    \n",
    "    conlleval.write_conll_file(true_labels, pred_labels, testSentences, models.idx2Label, eval_file) \n",
    "    p,r,f = conlleval.evaluate_conll_file(eval_file) \n",
    "    all_f1_scores.append(f)\n",
    "\n",
    "print(all_f1_scores)\n",
    "print(np.mean(all_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'conlleval' from '/srv/microNER/scripts/conlleval.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(conlleval)\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official script eval Germeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1st = models.idx2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 1, 'B-LOC': 7, 'I-LOC': 18, 'B-OTHpart': 2, 'B-PERderiv': 9, 'B-PER': 15, 'B-OTHderiv': 3, 'B-ORGpart': 6, 'B-LOCpart': 11, 'B-PERpart': 12, 'PADDING_TOKEN': 0, 'I-OTH': 16, 'B-ORG': 4, 'I-LOCderiv': 10, 'B-OTH': 13, 'B-ORGderiv': 5, 'I-PER': 8, 'B-LOCderiv': 17, 'I-ORG': 14}\n",
      "{0: 'PADDING_TOKEN', 1: 'I-LOC', 2: 'B-ORGderiv', 3: 'I-ORGderiv', 4: 'I-OTH', 5: 'I-ORGpart', 6: 'B-OTHderiv', 7: 'I-OTHpart', 8: 'I-PERderiv', 9: 'O', 10: 'B-PER', 11: 'B-PERderiv', 12: 'B-LOC', 13: 'I-PERpart', 14: 'B-ORGpart', 15: 'I-ORG', 16: 'B-LOCpart', 17: 'I-LOCpart', 18: 'B-ORG', 19: 'I-PER', 20: 'I-OTHderiv', 21: 'B-LOCderiv', 22: 'I-LOCderiv', 23: 'B-PERpart', 24: 'B-OTHpart', 25: 'B-OTH'}\n"
     ]
    }
   ],
   "source": [
    "print(models.label2Idx)\n",
    "print(idx_1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5938864628820961, 0.5281553398058253, 0.5590955806783146)\n"
     ]
    }
   ],
   "source": [
    "testSentences_2nd = utils.get_sentences_germeval('../../Resources/GermEVAL/NER-de-test.tsv', level2=True)\n",
    "load_model_indexes('model_lstm_germeval_2nd-level.h5.indexes')\n",
    "finalmodel_2nd = load_model('model_lstm_germeval_2nd-level.h5', custom_objects=create_custom_objects())\n",
    "true_labels_2nd, pred_labels_2nd = utils.predict_sequences(finalmodel_2nd, testSentences_2nd)\n",
    "print(compute_f1(pred_labels_2nd, true_labels_2nd, models.idx2Label))\n",
    "idx_2nd = models.idx2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = 'test_pl.tsv'\n",
    "conlleval.write_germeval_file(true_labels, pred_labels, true_labels_2nd, pred_labels_2nd, testSentences, idx_1st, idx_2nd, eval_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
