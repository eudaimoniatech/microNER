{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwiedemann/miniconda3/envs/kerasenv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "# from prepro import readfile,createBatches,createMatrices,iterate_minibatches,addCharInformatioin,padding\n",
    "from keras.utils import plot_model,Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    return caseLookup[casing]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing all deriv and part to misc. with BIO\n",
    "def modify_labels(dataset):\n",
    "    bad_labels = ['I-PERderiv','I-OTHpart','B-ORGderiv', 'I-OTH','B-OTHpart','B-LOCderiv','I-LOCderiv','I-OTHderiv','B-PERderiv','B-OTHderiv','B-PERpart','I-PERpart','I-LOCpart','B-LOCpart','I-ORGpart','I-ORGderiv','B-ORGpart','B-OTH']\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            label = word[1]\n",
    "            if label in bad_labels:\n",
    "                first_char = label[0]\n",
    "                if first_char == 'B' :\n",
    "                    word[1] = 'B-MISC'\n",
    "                else:\n",
    "                    word[1] = 'I-MISC'\n",
    "    return dataset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_germeval(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            \n",
    "            line = line.strip()\n",
    "            \n",
    "            # append sentence\n",
    "            if len(line) == 0:\n",
    "                if len(sentence):\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            \n",
    "            # get sentence tokens\n",
    "            splits = line.split()\n",
    "            if splits[0] == '#':\n",
    "                continue\n",
    "            temp = [splits[1], splits[3], splits[2]]\n",
    "            sentence.append(temp)\n",
    "        \n",
    "        # append last\n",
    "        if len(sentence):\n",
    "            sentences.append(sentence)    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preproecessing data from Conll\n",
    "def get_sentences_conll(filename):\n",
    "    '''\n",
    "        -DOCSTART- -X- -X- O\n",
    "\n",
    "    EU NNP B-NP B-ORG\n",
    "    rejects VBZ B-VP O\n",
    "    German JJ B-NP B-MISC\n",
    "    call NN I-NP O\n",
    "    to TO B-VP O\n",
    "    boycott VB I-VP O\n",
    "    British JJ B-NP B-MISC\n",
    "    lamb NN I-NP O\n",
    "    . . O O\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    read file\n",
    "    return format :\n",
    "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
    "    '''\n",
    "    f = open(filename,'rb')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        splits = line.split()\n",
    "        try:\n",
    "            word=splits[0].decode()\n",
    "            if word=='-DOCSTART-':\n",
    "                continue\n",
    "            label=splits[-1].decode()\n",
    "            temp=[word,label]\n",
    "            sentence.append(temp)\n",
    "        except Exception as e:\n",
    "            if len(sentence)!=0:\n",
    "                sentences.append(sentence)\n",
    "                sentence=[]\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "2200\n",
      "5100\n"
     ]
    }
   ],
   "source": [
    "trainSentences = get_sentences_germeval('../data/GermEVAL/NER-de-train.tsv')\n",
    "devSentences = get_sentences_germeval('../data/GermEVAL/NER-de-dev.tsv')\n",
    "testSentences = get_sentences_germeval('../data/GermEVAL/NER-de-test.tsv')\n",
    "\n",
    "# trainSentences = get_sentences('../data/CONLL/deu/deu_utf.train')\n",
    "# devSentences = get_sentences('../data/CONLL/deu/deu_utf.testa')\n",
    "# testSentences = get_sentences('../data/CONLL/deu/deu_utf.testb')\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O', 'O'], ['bis', 'O', 'O'], ['1953', 'O', 'O'], ['wurde', 'O', 'O'], ['der', 'O', 'O'], ['n√∂rdliche', 'O', 'O'], ['Teil', 'O', 'O'], ['als', 'O', 'O'], ['Jugendburg', 'O', 'O'], ['des', 'O', 'O'], ['Kolpingwerkes', 'O', 'B-OTH'], ['gebaut', 'O', 'O'], ['.', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "characters= set()\n",
    "labelL1Set = set()\n",
    "\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for word, label, L1 in sentence:\n",
    "            for char in word:\n",
    "                characters.add(char)\n",
    "            labelSet.add(label)\n",
    "            labelL1Set.add(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(labelSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Create a mapping for the labels ::\n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "    \n",
    "labelL12Idx = {}\n",
    "for label in labelL1Set:\n",
    "    labelL12Idx[label] = len(labelL12Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-ORGpart': 2, 'B-LOCderiv': 14, 'I-ORG': 3, 'B-PER': 4, 'B-ORG': 5, 'B-ORGderiv': 6, 'B-OTHderiv': 7, 'B-OTH': 8, 'B-PERderiv': 9, 'B-OTHpart': 10, 'B-LOCpart': 1, 'I-OTH': 11, 'I-PER': 12, 'I-LOC': 13, 'B-LOC': 16, 'I-LOCderiv': 17, 'B-PERpart': 15}\n",
      "{'B-LOCpart': 11, 'B-ORGpart': 0, 'B-OTHpart': 12, 'I-OTH': 1, 'I-PER': 13, 'I-LOC': 14, 'I-OTHpart': 10, 'I-ORGpart': 2, 'O': 15, 'I-OTHderiv': 3, 'I-ORG': 16, 'B-PER': 4, 'I-PERpart': 17, 'B-ORG': 18, 'B-ORGderiv': 19, 'B-OTHderiv': 5, 'B-OTH': 20, 'B-PERderiv': 6, 'I-PERderiv': 21, 'I-ORGderiv': 7, 'B-PERpart': 8, 'B-LOCderiv': 22, 'B-LOC': 9, 'I-LOCderiv': 23, 'I-LOCpart': 24}\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(label2Idx)\n",
    "print(labelL12Idx)\n",
    "L1embeddings = np.identity(len(labelL12Idx), dtype='float32')\n",
    "print(L1embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :: Hard coded case lookup ::\n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "{'other': 4, 'PADDING_TOKEN': 7, 'contains_digit': 6, 'allLower': 1, 'initialUpper': 3, 'numeric': 0, 'allUpper': 2, 'mainly_numeric': 5}\n"
     ]
    }
   ],
   "source": [
    "print(caseEmbeddings)\n",
    "print(case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['√ºberzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ª': 0, '√Ü': 1, 'ÂÜ≤': 256, 'p': 2, '‚Ä∫': 276, '√Ç': 274, '[': 3, '–£': 4, 'ÿ£': 216, '7': 6, '√Å': 7, '–∑': 8, '√é': 327, '—Ç': 9, '‚âò': 271, '√µ': 12, '≈•': 104, '~': 13, '≈Ñ': 15, '≈ë': 17, '0': 16, 'Œª': 108, '–õ': 19, '·∫ø': 20, '<': 21, '≈û': 242, 'ƒÖ': 22, '/': 23, 'H': 25, '–π': 26, 'ÊüØ': 55, '¬∏': 27, 'Œ≠': 32, '¬Ω': 31, 'e': 30, 'v': 33, '–∞': 35, 'ÿß': 36, 'F': 37, '\\x95': 38, '‚Äú': 39, '(': 325, '\\x92': 41, '¬±': 42, 'X': 43, '–æ': 308, '‚Äò': 226, '#': 45, '–±': 116, 'w': 46, 'È∑π': 5, '=': 263, '„Ç™': 47, 'G': 277, '–®': 48, 'ƒì': 49, '–≥': 212, 'Œ∫': 50, 'ƒÅ': 52, 'I': 53, 'q': 54, 'Â§™': 56, '·ø¶': 57, '‚äÉ': 285, '+': 58, '≈ô': 60, 'œÇ': 62, 'S': 61, '-': 117, '–ú': 282, 'ÂÆà': 65, '¬¥': 64, 'ÂÇ≥': 70, 'ƒô': 67, '‚Äì': 68, '„É©': 69, 'Á´†': 71, 'ƒ†': 273, 'Î≥Ñ': 73, '\\x94': 74, '–ª': 75, 'g': 76, '‚Äî': 324, '√∏': 230, '√°': 281, 'n': 77, '—î': 78, \"'\": 79, '√ñ': 80, '‰πù': 292, 'œÅ': 82, 'ƒ∞': 83, '9': 259, '\\u200e': 10, '—ä': 85, '√∫': 290, '‚Äö': 86, '¬∞': 87, '‰Ωê': 126, 'Âà•': 93, 'c': 90, '–í': 91, '–¢': 11, 'Âçó': 94, '≈à': 95, 'Q': 63, '>': 101, '`': 99, 'Œµ': 100, '¬π': 235, '√Ω': 102, '@': 103, '√ò': 209, 'z': 14, '√ª': 106, 'È∂¥': 18, 'h': 107, 'u': 66, 'ƒ´': 122, '‚Ä†': 110, '!': 114, 'U': 112, 'r': 113, '$': 105, '‚Äû': 118, 'Ÿä': 119, '√™': 238, 'Îèô': 120, '\\x96': 121, '≈º': 298, 'Ë°ì': 123, 'ÈÄ†': 125, 'o': 127, 'œÜ': 128, 'Œ±': 129, 'Œõ': 130, '—å': 131, 'b': 132, '6': 133, '·∏≥': 134, 'J': 135, '—É': 51, 'Œø': 136, '‚Ä≥': 137, '√ß': 72, '?': 138, '—ã': 140, '8': 142, 'Œ∑': 146, 'ÎåÄ': 147, 'ƒã': 265, '—ó': 152, '√º': 149, '*': 153, 'Œ¨': 253, 'm': 154, '.': 148, '≈è': 158, 'ÿ®': 157, 'ƒÉ': 24, '\\x80': 159, 'ƒ±': 160, '¬∑': 161, 'Âè∞': 162, '‚ãÖ': 291, '1': 302, ']': 188, '–¥': 163, 'A': 164, '3': 165, '≈Ü': 166, '√¶': 167, '√£': 168, 'Êáø': 169, '√•': 244, 'œå': 170, '‚ñ™': 171, '‚â§': 246, 'B': 172, '}': 173, 'Âçö': 174, '√Ñ': 175, 'i': 176, '\\x9a': 177, 'ÁÆó': 178, 'Ê®ì': 179, 'ÊÆø': 180, 'Î£®': 181, '≈ç': 182, '\\xad': 248, '√Ä': 183, '≈°': 28, '—é': 184, '¬©': 185, '≈´': 186, '≈†': 187, '%': 29, 'Êù±': 189, '√®': 190, '√∂': 191, '≈í': 192, '≈Ω': 193, '‚Äπ': 194, 'Œ≥': 251, '¬≤': 195, '√ó': 196, '—Å': 197, '–°': 198, '‚Äô': 199, '·ªá': 200, 'ƒ©': 310, '–ø': 81, '¬ª': 201, '√†': 203, '√≥': 139, '–ò': 304, '√¢': 204, 'V': 115, 'D': 205, '–ü': 207, 'd': 208, '–∂': 227, '≈Ç': 144, 's': 210, 'N': 211, 'Â£´': 215, 'L': 213, 'œÉ': 214, 'Ë≤¥': 218, '≈∫': 217, '_': 219, '¬£': 220, 'j': 143, 'ŒΩ': 222, '¬≥': 223, 'Ìïô': 224, 'Z': 225, 'ÿ∂': 145, 't': 228, ';': 229, '–∏': 84, '–ï': 232, '√∞': 233, '‚Äù': 234, 'y': 236, '¬´': 237, '¬µ': 40, '≈ª': 239, '—Ä': 240, 'ÂØù': 241, '√â': 231, '√¥': 243, 'R': 150, 'œà': 221, '√´': 151, 'x': 247, '—è': 249, '–∫': 250, 'œÖ': 254, '‚Ä¶': 252, 'ƒå': 255, '√Æ': 124, '¬ß': 92, '¬§': 245, '\\x99': 257, 'M': 88, ')': 258, '…®': 313, '–¶': 89, '≈ü': 202, 'ƒß': 267, ',': 266, '√≠': 268, 'Y': 269, '√±': 272, 'ÂÖ¨': 275, 'UNKNOWN': 328, 'T': 260, '5': 155, '√ú': 279, 'f': 44, '\"': 280, '–µ': 156, 'ŸÄ': 109, 'ƒç': 262, 'Œ≤': 284, 'Â¶É': 321, 'œÑ': 286, '‚àí': 287, '–≤': 288, 'E': 289, '≈æ': 301, '≈Å': 293, '√§': 306, 'P': 96, '—Ö': 295, 'C': 296, '≈ì': 297, 'l': 97, '√û': 299, '·∏´': 300, '‚Üí': 264, '√à': 206, '≈õ': 303, '4': 261, 'œÄ': 294, 'W': 305, '√©': 59, ':': 307, '‚Äê': 309, 'ƒá': 270, '√Ö': 311, '&': 312, 'k': 98, 'K': 314, '‚Ç¨': 315, '–º': 316, 'ƒõ': 317, 'a': 318, 'Œπ': 319, '«í': 320, 'O': 141, 'Œ†': 322, 'Êùé': 323, '–Ω': 283, 'ƒü': 34, 'ŸÜ': 326, '√ü': 278, '2': 111}\n"
     ]
    }
   ],
   "source": [
    "char2Idx={}\n",
    "for char in characters:\n",
    "    char2Idx[char] = len(char2Idx)\n",
    "char2Idx['UNKNOWN'] = len(char2Idx)\n",
    "print(char2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['√ºberzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "# ft = fastText.load_model(\"../embeddings/cc.de.300.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "print(nb_embedding_dims)\n",
    "print(len(trainSentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Schartau', 'O'], ['sagte', 'O'], ['dem', 'O'], ['\"', 'O'], ['Tagesspiegel', 'O'], ['\"', 'O'], ['vom', 'O'], ['Freitag', 'O'], [',', 'O'], ['Fischer', 'O'], ['sei', 'O'], ['\"', 'O'], ['in', 'O'], ['einer', 'O'], ['Weise', 'O'], ['aufgetreten', 'O'], [',', 'O'], ['die', 'O'], ['alles', 'O'], ['andere', 'O'], ['als', 'O'], ['√ºberzeugend', 'O'], ['war', 'O'], ['\"', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(trainSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "train_batches = createBatches(trainSentences)\n",
    "dev_batches = createBatches(devSentences)\n",
    "test_batches = createBatches(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "3\n",
      "[[['Alles', 'O', 'O'], ['richtig', 'O', 'O'], ['.', 'O', 'O']], [['Farben', 'O', 'O'], ['eingef√ºhrt', 'O', 'O'], ['.', 'O', 'O']], [['Material', 'O', 'O'], ['gewinnen', 'O', 'O'], ['.', 'O', 'O']]]\n"
     ]
    }
   ],
   "source": [
    "batches = train_batches\n",
    "print(len(batches))\n",
    "print(len(batches[0]))\n",
    "print(batches[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(getCasing(\".\", case2Idx))\n",
    "print(L1embeddings[1])\n",
    "for input_data, output_data in generator(train_batches):\n",
    "    print(input_data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "            nerlevel1_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch\n",
    "                sentence = batch[index]\n",
    "    #             print(sentence)\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                temp_nerlevel1_embeddings=[]\n",
    "                for word in sentence:\n",
    "                    word, label, L1 = word\n",
    "                    casing = getCasing(word, case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in char2Idx.keys():\n",
    "                            temp_char2.append(char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    word_vector = ft.get_word_vector(word.lower())\n",
    "                    # word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(label2Idx[label])\n",
    "                    temp_nerlevel1_embeddings.append(labelL12Idx[L1])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                nerlevel1_embeddings.append(temp_nerlevel1_embeddings)\n",
    "                \n",
    "                temp_output = to_categorical(temp_output, len(label2Idx))\n",
    "                output_labels.append(temp_output)\n",
    "    #             output_labels = to_categorical()\n",
    "    #             output_labels = np.array(output_labels)\n",
    "    #             output_labels = output_labels[...,newaxis]\n",
    "\n",
    "    #             print(np.array(word_embeddings).shape)\n",
    "    #             print(np.array(case_embeddings).shape)\n",
    "    #             print(np.array(char_embeddings).shape)\n",
    "    #             print(output_labels.shape)\n",
    "    #             print(\"******************\\n\\n\")\n",
    "            yield ([np.array(word_embeddings), np.array(nerlevel1_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == len(batch):\n",
    "            break\n",
    "    return(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "idx2LabelL1 = {v: k for k, v in labelL12Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for s in y_pred:\n",
    "        pred_labels.append(get_label_from_categorical(s))\n",
    "    for s in y_true:\n",
    "        true_labels.append(get_label_from_categorical(s))\n",
    "    p, r, f = compute_f1(y_pred, y_true, idx2Label)\n",
    "    return r\n",
    "\n",
    "def get_model():\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    L1_input = Input(shape=(None,), dtype='int32', name='L1_input')\n",
    "    L1_embed = Embedding(output_dim=L1embeddings.shape[1], input_dim=L1embeddings.shape[0], weights=[L1embeddings], trainable=False, name = 'L1_embed')(L1_input)\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,52,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    kernel_sizes = (3, 4, 5)\n",
    "    conv_blocks = []\n",
    "    for sz in kernel_sizes:\n",
    "        conv = TimeDistributed(Conv1D(\n",
    "                             kernel_size=sz,\n",
    "                             filters=32,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1))(embed_char_out)\n",
    "        conv = TimeDistributed(MaxPooling1D(52))(conv)\n",
    "        conv = TimeDistributed(Flatten())(conv)\n",
    "        conv_blocks.append(conv)\n",
    "    output = concatenate([words_input, L1_embed, casing, conv_blocks[0], conv_blocks[1], conv_blocks[2]])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5))(output)\n",
    "    output = TimeDistributed(Dense(len(label2Idx)))(output)\n",
    "    crf = CRF(len(label2Idx))\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, L1_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)\n",
    "\n",
    "class F1History(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.f1_scores = []\n",
    "        self.max_f1 = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.acc.append(logs.get('val_acc'))\n",
    "        true_labels, pred_labels = predict_batches(dev_batches)\n",
    "        pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "        self.f1_scores.append(f1)\n",
    "        if epoch > 30 and f1 > self.max_f1:\n",
    "            print(\"\\nNew maximum F1 score: \" + str(f1) + \" (before: \" + str(self.max_f1) + \") Saving to \" + tmp_model_filename)\n",
    "            self.max_f1 = f1\n",
    "            model.save(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10528       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_90 (TimeDistri (None, None, 52, 32) 3104        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistri (None, None, 52, 32) 4128        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_96 (TimeDistri (None, None, 52, 32) 5152        char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "L1_input (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistri (None, None, 1, 32)  0           time_distributed_90[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistri (None, None, 1, 32)  0           time_distributed_93[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistri (None, None, 1, 32)  0           time_distributed_96[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "L1_embed (Embedding)            (None, None, 25)     625         L1_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_92 (TimeDistri (None, None, 32)     0           time_distributed_91[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistri (None, None, 32)     0           time_distributed_94[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistri (None, None, 32)     0           time_distributed_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, 429)    0           words_input[0][0]                \n",
      "                                                                 L1_embed[0][0]                   \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_92[0][0]        \n",
      "                                                                 time_distributed_95[0][0]        \n",
      "                                                                 time_distributed_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, None, 400)    1008000     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistri (None, None, 18)     7218        bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_9 (CRF)                     (None, None, 18)     702         time_distributed_99[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,039,521\n",
      "Trainable params: 1,038,832\n",
      "Non-trainable params: 689\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/80\n",
      "51/51 [==============================] - 28s 556ms/step - loss: 0.1464 - acc: 0.9744 - val_loss: 0.0332 - val_acc: 0.9947\n",
      "Epoch 2/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: 0.0495 - acc: 0.9916 - val_loss: 0.0294 - val_acc: 0.9947\n",
      "Epoch 3/80\n",
      "51/51 [==============================] - 23s 446ms/step - loss: 0.0382 - acc: 0.9918 - val_loss: 0.0313 - val_acc: 0.9955\n",
      "Epoch 4/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: 0.0272 - acc: 0.9926 - val_loss: 0.0308 - val_acc: 0.9938\n",
      "Epoch 5/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0055 - val_acc: 0.9958\n",
      "Epoch 6/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: 0.0113 - acc: 0.9942 - val_loss: 0.0714 - val_acc: 0.9777\n",
      "Epoch 7/80\n",
      "51/51 [==============================] - 22s 434ms/step - loss: 0.0049 - acc: 0.9949 - val_loss: 0.0212 - val_acc: 0.9879\n",
      "Epoch 8/80\n",
      "51/51 [==============================] - 22s 426ms/step - loss: -7.5192e-04 - acc: 0.9950 - val_loss: 0.0134 - val_acc: 0.9902\n",
      "Epoch 9/80\n",
      "51/51 [==============================] - 22s 430ms/step - loss: -0.0065 - acc: 0.9958 - val_loss: -0.0127 - val_acc: 0.9961\n",
      "Epoch 10/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.0127 - acc: 0.9964 - val_loss: -0.0168 - val_acc: 0.9964\n",
      "Epoch 11/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.0162 - acc: 0.9963 - val_loss: -0.0224 - val_acc: 0.9964\n",
      "Epoch 12/80\n",
      "51/51 [==============================] - 22s 441ms/step - loss: -0.0217 - acc: 0.9966 - val_loss: -0.0274 - val_acc: 0.9966\n",
      "Epoch 13/80\n",
      "51/51 [==============================] - 23s 453ms/step - loss: -0.0275 - acc: 0.9971 - val_loss: -0.0322 - val_acc: 0.9968\n",
      "Epoch 14/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.0319 - acc: 0.9970 - val_loss: -0.0313 - val_acc: 0.9959\n",
      "Epoch 15/80\n",
      "51/51 [==============================] - 22s 425ms/step - loss: -0.0364 - acc: 0.9972 - val_loss: -0.0421 - val_acc: 0.9971\n",
      "Epoch 16/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.0412 - acc: 0.9974 - val_loss: -0.0410 - val_acc: 0.9963\n",
      "Epoch 17/80\n",
      "51/51 [==============================] - 22s 430ms/step - loss: -0.0450 - acc: 0.9972 - val_loss: -0.0493 - val_acc: 0.9970\n",
      "Epoch 18/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0498 - acc: 0.9974 - val_loss: -0.0547 - val_acc: 0.9969\n",
      "Epoch 19/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.0541 - acc: 0.9975 - val_loss: -0.0600 - val_acc: 0.9972\n",
      "Epoch 20/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.0588 - acc: 0.9977 - val_loss: -0.0561 - val_acc: 0.9944\n",
      "Epoch 21/80\n",
      "51/51 [==============================] - 23s 445ms/step - loss: -0.0619 - acc: 0.9975 - val_loss: -0.0634 - val_acc: 0.9965\n",
      "Epoch 22/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.0655 - acc: 0.9972 - val_loss: -0.0679 - val_acc: 0.9954\n",
      "Epoch 23/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.0710 - acc: 0.9976 - val_loss: -0.0739 - val_acc: 0.9968\n",
      "Epoch 24/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0758 - acc: 0.9977 - val_loss: -0.0799 - val_acc: 0.9969\n",
      "Epoch 25/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.0808 - acc: 0.9980 - val_loss: -0.0861 - val_acc: 0.9970\n",
      "Epoch 26/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0850 - acc: 0.9979 - val_loss: -0.0891 - val_acc: 0.9968\n",
      "Epoch 27/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.0894 - acc: 0.9980 - val_loss: -0.0945 - val_acc: 0.9969\n",
      "Epoch 28/80\n",
      "51/51 [==============================] - 23s 449ms/step - loss: -0.0940 - acc: 0.9981 - val_loss: -0.1000 - val_acc: 0.9970\n",
      "Epoch 29/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.0984 - acc: 0.9982 - val_loss: -0.1044 - val_acc: 0.9971\n",
      "Epoch 30/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.1025 - acc: 0.9982 - val_loss: -0.1073 - val_acc: 0.9963\n",
      "Epoch 31/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1065 - acc: 0.9982 - val_loss: -0.1128 - val_acc: 0.9970\n",
      "Epoch 32/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1127 - acc: 0.9983\n",
      "New maximum F1 score: 0.6631016042780749 (before: 0) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 25s 481ms/step - loss: -0.1113 - acc: 0.9983 - val_loss: -0.1174 - val_acc: 0.9971\n",
      "Epoch 33/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1174 - acc: 0.9984\n",
      "New maximum F1 score: 0.6777251184834123 (before: 0.6631016042780749) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1160 - acc: 0.9984 - val_loss: -0.1227 - val_acc: 0.9971\n",
      "Epoch 34/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: -0.1204 - acc: 0.9985 - val_loss: -0.1232 - val_acc: 0.9966\n",
      "Epoch 35/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1247 - acc: 0.9985 - val_loss: -0.1309 - val_acc: 0.9971\n",
      "Epoch 36/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1307 - acc: 0.9985\n",
      "New maximum F1 score: 0.6791443850267381 (before: 0.6777251184834123) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 23s 447ms/step - loss: -0.1291 - acc: 0.9985 - val_loss: -0.1357 - val_acc: 0.9972\n",
      "Epoch 37/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1335 - acc: 0.9985 - val_loss: -0.1405 - val_acc: 0.9972\n",
      "Epoch 38/80\n",
      "51/51 [==============================] - 23s 443ms/step - loss: -0.1379 - acc: 0.9987 - val_loss: -0.1443 - val_acc: 0.9971\n",
      "Epoch 39/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1421 - acc: 0.9987 - val_loss: -0.1486 - val_acc: 0.9971\n",
      "Epoch 40/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1484 - acc: 0.9987\n",
      "New maximum F1 score: 0.6810810810810811 (before: 0.6791443850267381) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 23s 444ms/step - loss: -0.1466 - acc: 0.9987 - val_loss: -0.1530 - val_acc: 0.9973\n",
      "Epoch 41/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.1507 - acc: 0.9986 - val_loss: -0.1572 - val_acc: 0.9970\n",
      "Epoch 42/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1570 - acc: 0.9987\n",
      "New maximum F1 score: 0.6878048780487804 (before: 0.6810810810810811) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.1551 - acc: 0.9987 - val_loss: -0.1633 - val_acc: 0.9972\n",
      "Epoch 43/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1596 - acc: 0.9988 - val_loss: -0.1671 - val_acc: 0.9972\n",
      "Epoch 44/80\n",
      "51/51 [==============================] - 22s 428ms/step - loss: -0.1640 - acc: 0.9988 - val_loss: -0.1718 - val_acc: 0.9971\n",
      "Epoch 45/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1705 - acc: 0.9989\n",
      "New maximum F1 score: 0.694789081885856 (before: 0.6878048780487804) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1684 - acc: 0.9989 - val_loss: -0.1772 - val_acc: 0.9972\n",
      "Epoch 46/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.1728 - acc: 0.9989 - val_loss: -0.1809 - val_acc: 0.9970\n",
      "Epoch 47/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1769 - acc: 0.9988 - val_loss: -0.1859 - val_acc: 0.9971\n",
      "Epoch 48/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.1815 - acc: 0.9990 - val_loss: -0.1904 - val_acc: 0.9971\n",
      "Epoch 49/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1858 - acc: 0.9990 - val_loss: -0.1946 - val_acc: 0.9971\n",
      "Epoch 50/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.1902 - acc: 0.9990 - val_loss: -0.1994 - val_acc: 0.9971\n",
      "Epoch 51/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.1970 - acc: 0.9991\n",
      "New maximum F1 score: 0.6982543640897756 (before: 0.694789081885856) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1946 - acc: 0.9991 - val_loss: -0.2037 - val_acc: 0.9972\n",
      "Epoch 52/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.1990 - acc: 0.9991 - val_loss: -0.2078 - val_acc: 0.9969\n",
      "Epoch 53/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.2031 - acc: 0.9990 - val_loss: -0.2105 - val_acc: 0.9971\n",
      "Epoch 54/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2073 - acc: 0.9990 - val_loss: -0.2164 - val_acc: 0.9969\n",
      "Epoch 55/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2117 - acc: 0.9991 - val_loss: -0.2212 - val_acc: 0.9970\n",
      "Epoch 56/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2188 - acc: 0.9991\n",
      "New maximum F1 score: 0.7024390243902439 (before: 0.6982543640897756) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2161 - acc: 0.9991 - val_loss: -0.2261 - val_acc: 0.9973\n",
      "Epoch 57/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.2207 - acc: 0.9993 - val_loss: -0.2306 - val_acc: 0.9971\n",
      "Epoch 58/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.2249 - acc: 0.9992 - val_loss: -0.2348 - val_acc: 0.9972\n",
      "Epoch 59/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2322 - acc: 0.9993\n",
      "New maximum F1 score: 0.7072599531615924 (before: 0.7024390243902439) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.2293 - acc: 0.9993 - val_loss: -0.2400 - val_acc: 0.9973\n",
      "Epoch 60/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2336 - acc: 0.9993 - val_loss: -0.2438 - val_acc: 0.9972\n",
      "Epoch 61/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2379 - acc: 0.9993 - val_loss: -0.2475 - val_acc: 0.9972\n",
      "Epoch 62/80\n",
      "51/51 [==============================] - 22s 432ms/step - loss: -0.2421 - acc: 0.9993 - val_loss: -0.2518 - val_acc: 0.9970\n",
      "Epoch 63/80\n",
      "51/51 [==============================] - 22s 440ms/step - loss: -0.2457 - acc: 0.9991 - val_loss: -0.2559 - val_acc: 0.9966\n",
      "Epoch 64/80\n",
      "51/51 [==============================] - 22s 431ms/step - loss: -0.2502 - acc: 0.9991 - val_loss: -0.2623 - val_acc: 0.9972\n",
      "Epoch 65/80\n",
      "51/51 [==============================] - 23s 444ms/step - loss: -0.2550 - acc: 0.9993 - val_loss: -0.2648 - val_acc: 0.9967\n",
      "Epoch 66/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.2594 - acc: 0.9994 - val_loss: -0.2707 - val_acc: 0.9973\n",
      "Epoch 67/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.2670 - acc: 0.9994\n",
      "New maximum F1 score: 0.7135922330097088 (before: 0.7072599531615924) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.2637 - acc: 0.9994 - val_loss: -0.2755 - val_acc: 0.9974\n",
      "Epoch 68/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2680 - acc: 0.9994 - val_loss: -0.2797 - val_acc: 0.9973\n",
      "Epoch 69/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.2723 - acc: 0.9995 - val_loss: -0.2846 - val_acc: 0.9973\n",
      "Epoch 70/80\n",
      "51/51 [==============================] - 22s 438ms/step - loss: -0.2767 - acc: 0.9995 - val_loss: -0.2882 - val_acc: 0.9972\n",
      "Epoch 71/80\n",
      "51/51 [==============================] - 22s 433ms/step - loss: -0.2810 - acc: 0.9995 - val_loss: -0.2927 - val_acc: 0.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/80\n",
      "51/51 [==============================] - 22s 429ms/step - loss: -0.2852 - acc: 0.9995 - val_loss: -0.2966 - val_acc: 0.9969\n",
      "Epoch 73/80\n",
      "51/51 [==============================] - 23s 441ms/step - loss: -0.2893 - acc: 0.9995 - val_loss: -0.3011 - val_acc: 0.9974\n",
      "Epoch 74/80\n",
      "51/51 [==============================] - 22s 436ms/step - loss: -0.2936 - acc: 0.9994 - val_loss: -0.3063 - val_acc: 0.9971\n",
      "Epoch 75/80\n",
      "51/51 [==============================] - 23s 450ms/step - loss: -0.2980 - acc: 0.9995 - val_loss: -0.3100 - val_acc: 0.9971\n",
      "Epoch 76/80\n",
      "51/51 [==============================] - 23s 446ms/step - loss: -0.3020 - acc: 0.9993 - val_loss: -0.3112 - val_acc: 0.9971\n",
      "Epoch 77/80\n",
      "51/51 [==============================] - 22s 435ms/step - loss: -0.3049 - acc: 0.9991 - val_loss: -0.3189 - val_acc: 0.9972\n",
      "Epoch 78/80\n",
      "50/51 [============================>.] - ETA: 0s - loss: -0.3143 - acc: 0.9994\n",
      "New maximum F1 score: 0.7233009708737864 (before: 0.7135922330097088) Saving to tmp_generator_NER_best.h5\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.3104 - acc: 0.9994 - val_loss: -0.3254 - val_acc: 0.9974\n",
      "Epoch 79/80\n",
      "51/51 [==============================] - 22s 437ms/step - loss: -0.3151 - acc: 0.9995 - val_loss: -0.3296 - val_acc: 0.9973\n",
      "Epoch 80/80\n",
      "51/51 [==============================] - 22s 439ms/step - loss: -0.3191 - acc: 0.9995 - val_loss: -0.3278 - val_acc: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f755c7cbcc0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "# checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "history = F1History()\n",
    "model = get_model()\n",
    "model.fit_generator(\n",
    "    generator(train_batches), \n",
    "    epochs = 80, steps_per_epoch = len(train_batches), \n",
    "    validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "    callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9946695905111053, 0.9946695905111053, 0.9753653725981712, 0.971133625805378, 0.9892300726066936, 0.9916527701778846, 0.9934801145033403, 0.9193957837332378, 0.9945173840360207, 0.9694857255708088, 0.9949436785416169, 0.9296631378206339, 0.9950203010981733, 0.9947294317050414, 0.9950850945711136, 0.9952510368282145, 0.9952144384113225, 0.9952569980783896, 0.9953162610530853, 0.9944079532135617, 0.995281142429872, 0.9953866038810123, 0.9953606687350707, 0.9954716947945681, 0.9955632332509214, 0.995618404041637, 0.9956095008416609, 0.9956665044752034, 0.9953894490545446, 0.9953945659778335, 0.995706004283645, 0.9957643909887834, 0.9956305682930079, 0.9957253030213443, 0.9958990923112089, 0.9957777957211841, 0.9955365413427353, 0.9957157587734136, 0.9956198622421785, 0.9956063252958385, 0.9958345579559152, 0.9956996938586236, 0.9958652172847228, 0.9959894129092043, 0.9959836213155226, 0.9959905016151341, 0.9959326248277317, 0.9959960159659386, 0.9962591779232025, 0.9959572835402055, 0.9960178007591854, 0.9960247241637924, 0.9961614792726257, 0.9959337345307524, 0.9960361340641976, 0.9958904425664381, 0.9960068343715234, 0.9958731552145698, 0.9956828972697258, 0.995921416580677, 0.9959934198856354, 0.9959670436653224, 0.995522012385455, 0.9114039875702424, 0.9948186420581557, 0.9102330112186345, 0.9950387359478257, 0.9955177603526549, 0.9961365892670372, 0.9961044318567622, 0.9962850817225196, 0.9960857303305106, 0.9961196438832717, 0.9958124020695687, 0.9952916812354868, 0.9956907460906289, 0.9962496629628268, 0.9956224945458498, 0.9955316628922116, 0.9961185671524568]\n",
      "[0, 0, 0.1321455085374907, 0.11959287531806617, 0.2818181818181818, 0.30468749999999994, 0.33004926108374383, 0.06942889137737962, 0.2626262626262626, 0.15420200462606012, 0.10666666666666666, 0.052261306532663324, 0.208, 0.43271767810026385, 0.16239316239316237, 0.4194528875379939, 0.42633228840125387, 0.3223443223443223, 0.3475177304964539, 0.41791044776119407, 0.23387096774193547, 0.26666666666666666, 0.33935018050541516, 0.3525179856115108, 0.3206106870229008, 0.3542435424354244, 0.35766423357664234, 0.42857142857142855, 0.5169712793733682, 0.3394833948339484, 0.42384105960264895, 0.42483660130718953, 0.39024390243902435, 0.4662576687116564, 0.5120481927710843, 0.4899135446685879, 0.41558441558441556, 0.515759312320917, 0.45625000000000004, 0.3571428571428571, 0.47352024922118385, 0.5212464589235127, 0.4804804804804806, 0.5480225988700564, 0.5389221556886227, 0.49358974358974356, 0.5685279187817259, 0.5474860335195532, 0.5872576177285318, 0.5833333333333333, 0.5540166204986149, 0.5699208443271768, 0.5698324022346369, 0.5759162303664921, 0.5344352617079889, 0.5670886075949367, 0.5591397849462365, 0.5513513513513514, 0.5497630331753556, 0.588235294117647, 0.5728900255754475, 0.5750636132315521, 0.5268542199488491, 0.13911060433295325, 0.07272727272727272, 0.04491091042226019, 0.12875536480686695, 0.5576923076923078, 0.5929648241206029, 0.5873417721518989, 0.5633802816901409, 0.587378640776699, 0.5604395604395604, 0.5849056603773585, 0.5726681127982647, 0.580335731414868, 0.5922077922077922, 0.5700712589073633, 0.5740740740740741, 0.582089552238806]\n"
     ]
    }
   ],
   "source": [
    "print(history.acc)\n",
    "print(history.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(tmp_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "print(compute_f1(pred_labels, true_labels, idx2Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('results_wiki.txt', 'w')\n",
    "for run_i in range(13):\n",
    "    print(\"Run \" + str(run_i))\n",
    "    \n",
    "    tmp_model_filename = 'tmp_generator_NER_best.' + str(run_i) + '.h5'\n",
    "    # tmp_model_filename = 'tmp_generator_NER_best.h5'\n",
    "    # checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')\n",
    "    history = F1History()\n",
    "    \n",
    "    model = get_model()\n",
    "    model.fit_generator(\n",
    "        generator(train_batches), \n",
    "        epochs = 80, steps_per_epoch = len(train_batches), \n",
    "        validation_data = generator(dev_batches), validation_steps = len(dev_batches), \n",
    "        callbacks = [history]\n",
    "    )\n",
    "    \n",
    "    model.load_weights(tmp_model_filename)\n",
    "    true_labels, pred_labels = predict_batches(test_batches)\n",
    "    \n",
    "    pre, rec, f1 = compute_f1(pred_labels, true_labels, idx2Label)\n",
    "    f.write(str(run_i) + \"\\t\" + str(pre) + \"\\t\" + str(rec) +  \"\\t\" + str(f1))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, json\n",
    "# copy file for best run\n",
    "shutil.copyfile('tmp_generator_NER_best.0.h5', 'final_model_germeval.h5')\n",
    "with open(\"final_model_germeval.indexes\", \"w\") as f:\n",
    "    json.dump([idx2Label, label2Idx, char2Idx, case2Idx], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
