{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "from keras.utils import to_categorical\n",
    "from validation import compute_f1\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from numpy import newaxis\n",
    "import models\n",
    "import sklearn\n",
    "import subprocess\n",
    "import fastText\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(sentences, label2Idx, case2Idx, char2Idx):\n",
    "    #{'numeric': 0, 'allLower': 1, 'contains_digit': 6, 'PADDING_TOKEN': 7, 'other': 4, 'allUpper': 2, 'mainly_numeric': 5, 'initialUpper': 3}\n",
    "\n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        wordIndices = []\n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in sentence:  \n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                if x in models.char2Idx.keys():\n",
    "                    charIdx.append(models.char2Idx[x])\n",
    "                else:\n",
    "                    charIdx.append(models.char2Idx['UNKNOWN'])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(word)\n",
    "            caseIndices.append(utils.getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def padding(Sentences):\n",
    "    maxlen = 52\n",
    "    for sentence in Sentences:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
    "    return Sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dataset(dataset):\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes            \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    return predLabels, correctLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "2200\n",
      "5100\n"
     ]
    }
   ],
   "source": [
    "trainSentences = utils.get_sentences_germeval('../data/germeval/NER-de-train.tsv')\n",
    "devSentences = utils.get_sentences_germeval('../data/germeval/NER-de-dev.tsv')\n",
    "testSentences = utils.get_sentences_germeval('../data/germeval/NER-de-test.tsv')\n",
    "\n",
    "# trainSentences.extend(utils.get_sentences_conll('../data/CONLL/deu/deu_utf.train.bio'))\n",
    "# devSentences.extend(utils.get_sentences_conll('../data/CONLL/deu/deu_utf.testa.bio'))\n",
    "# testSentences.extend(utils.get_sentences_conll('../data/CONLL/deu/deu_utf.testb.bio'))\n",
    "\n",
    "print(len(trainSentences))\n",
    "print(len(devSentences))\n",
    "print(len(testSentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.max_sequence_length = 56\n",
    "# TODO replace with non-hardcoded version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1951', 'O'], ['bis', 'O'], ['1953', 'O'], ['wurde', 'O'], ['der', 'O'], ['nördliche', 'O'], ['Teil', 'O'], ['als', 'O'], ['Jugendburg', 'O'], ['des', 'O'], ['Kolpingwerkes', 'B-OTH'], ['gebaut', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(testSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load label mapping\n",
    "import json\n",
    "indexMappings = json.load(open(\"model_lstm_germeval_v2.0.h5.indexes\", \"r\"))\n",
    "models.idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "models.label2Idx = indexMappings[1]\n",
    "models.char2Idx = indexMappings[2]\n",
    "models.case2Idx = indexMappings[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'PADDING_TOKEN', 1: 'I-LOC', 2: 'B-ORGderiv', 3: 'I-ORGderiv', 4: 'I-OTH', 5: 'I-ORGpart', 6: 'B-OTHderiv', 7: 'I-OTHpart', 8: 'I-PERderiv', 9: 'O', 10: 'B-PER', 11: 'B-PERderiv', 12: 'B-LOC', 13: 'I-PERpart', 14: 'B-ORGpart', 15: 'I-ORG', 16: 'B-LOCpart', 17: 'I-LOCpart', 18: 'B-ORG', 19: 'I-PER', 20: 'I-OTHderiv', 21: 'B-LOCderiv', 22: 'I-LOCderiv', 23: 'B-PERpart', 24: 'B-OTHpart', 25: 'B-OTH'}\n",
      "{'I-LOC': 1, 'B-ORGderiv': 2, 'I-ORG': 15, 'B-LOCpart': 16, 'B-ORGpart': 14, 'I-ORGderiv': 3, 'B-LOCderiv': 21, 'I-LOCpart': 17, 'B-ORG': 18, 'I-ORGpart': 5, 'B-PERpart': 23, 'B-OTHderiv': 6, 'I-OTHpart': 7, 'I-PERderiv': 8, 'O': 9, 'I-LOCderiv': 22, 'I-PER': 19, 'PADDING_TOKEN': 0, 'B-PER': 10, 'I-OTHderiv': 20, 'B-PERderiv': 11, 'I-OTH': 4, 'B-OTH': 25, 'B-LOC': 12, 'I-PERpart': 13, 'B-OTHpart': 24}\n",
      "{'Å': 5, '[': 121, 'Ü': 6, 'к': 7, 'e': 8, 'ю': 248, 'П': 234, 'L': 9, '±': 10, 'µ': 11, 'ø': 12, 'г': 19, 'l': 17, '-': 18, 'n': 20, '8': 279, '³': 22, 'ő': 200, 'q': 23, 'Þ': 25, 'ρ': 175, 'G': 26, 'œ': 323, 'ż': 27, '²': 281, 'д': 231, 'K': 28, 'е': 29, 'ť': 30, '別': 31, 'ḫ': 32, 'İ': 33, 'ệ': 34, '懿': 36, '£': 202, 'р': 37, '‹': 65, 'UNKNOWN': 333, '傳': 14, '\"': 331, '\\x99': 40, '©': 41, 'т': 244, 'ї': 43, 'T': 97, 'В': 277, 'ş': 236, 'á': 131, 'ا': 283, 'ο': 44, '<S>': 1, 'N': 45, 'h': 46, 'X': 48, 'D': 49, 'Ø': 57, 'M': 52, 'ó': 53, '½': 55, 'έ': 56, '李': 59, '`': 61, 'ō': 63, 'I': 62, 'y': 307, '—': 66, 'ğ': 325, '柯': 21, '’': 304, 'V': 68, 'b': 69, ':': 67, '´': 71, 'υ': 73, 'ـ': 80, 'ψ': 311, '士': 77, 'ن': 82, 'є': 78, 'ě': 81, 'з': 83, 'Π': 84, 'ä': 129, 'â': 86, 'õ': 39, 'Œ': 88, '»': 89, 'Ġ': 180, ',': 90, 'd': 91, '_': 92, 'a': 93, 'E': 47, 'н': 72, '6': 94, 'i': 95, 'Â': 222, 'ř': 96, 'Ž': 104, ';': 99, '\\x9a': 108, 'j': 101, 'М': 102, 'ë': 103, ']': 106, 'î': 105, '術': 291, 'ь': 109, '⊃': 110, '@': 126, '<W>': 3, 'σ': 75, 'O': 111, 'オ': 24, '\\x96': 60, 'z': 113, '§': 118, 'W': 115, '(': 117, 'È': 120, '×': 172, 'б': 187, '▪': 122, '\\x95': 123, '‚': 13, 'U': 124, 'أ': 125, '†': 127, 'ῦ': 128, 'è': 130, '佐': 135, 'Š': 132, 'ι': 114, 'ö': 312, 'Λ': 173, 'F': 136, 'ā': 228, '博': 134, 'Ä': 137, '›': 138, 'ъ': 139, '>': 142, 'ą': 298, 'P': 143, '⋅': 144, 'å': 247, '<': 147, '章': 85, '造': 119, 'ي': 148, 'ã': 149, '鶴': 156, '7': 152, 'f': 141, '鷹': 154, 'Т': 155, 'ǒ': 157, 'ʻ': 158, 'ĩ': 160, 'α': 161, '2': 87, 'ċ': 151, 'ū': 245, 'g': 162, '학': 163, 'ı': 58, '$': 16, 'ę': 166, '≘': 253, 'o': 167, 'ɨ': 168, 'ł': 170, 'ž': 171, 'ħ': 174, 'ς': 146, '寝': 176, 'A': 177, 'Ż': 208, 'ð': 181, 'в': 182, 'Î': 183, '동': 184, 'ض': 185, 'х': 186, 'ē': 330, '*': 188, '\\x94': 133, 'Ш': 189, 'J': 190, 'H': 191, 'Ł': 300, 'ά': 239, 'v': 204, '貴': 193, 'ế': 194, '南': 195, 's': 196, 'Æ': 251, 'r': 197, 'о': 198, 'й': 199, '.': 201, 'p': 203, 'С': 178, 'γ': 206, 'À': 207, 'Á': 150, '‘': 209, 'S': 210, 'Y': 211, 'κ': 289, 'É': 70, '대': 212, '„': 213, '\\xad': 153, 'Q': 215, '\\u200e': 216, '%': 217, '殿': 35, '→': 219, 'm': 140, 'ń': 221, '</S>': 2, 'ό': 223, 'Ц': 224, 'у': 265, 'ă': 256, '9': 227, 'ラ': 76, 'Ö': 15, 'ç': 230, 'ī': 164, '?': 232, 'k': 233, 'æ': 242, 'ô': 235, 'И': 226, '–': 237, '/': 238, 'л': 240, '“': 241, '5': 243, 'ú': 64, '\\x80': 314, 'м': 246, 'ν': 38, 'í': 116, 'ж': 287, 'ß': 98, '&': 317, 'ý': 268, 'и': 250, '算': 252, 'Z': 316, '冲': 107, 'u': 254, '€': 255, '…': 257, 'c': 100, '1': 214, 'ņ': 260, '#': 261, 'à': 262, 'š': 264, '°': 42, 'é': 266, '妃': 326, 'φ': 267, 'R': 269, '=': 271, 'ć': 270, '!': 272, '東': 218, 'ü': 273, 'w': 274, 'Е': 324, 'ź': 276, 'а': 220, '公': 259, '¤': 258, 'PADDING_TOKEN': 0, 'ε': 280, 'ы': 308, '~': 282, 'ḳ': 179, '‐': 328, 'x': 284, '台': 285, 'λ': 327, 't': 286, 'C': 275, '</W>': 4, '}': 290, '樓': 293, '별': 225, 'B': 294, 'η': 295, '”': 296, 'Л': 297, 'ب': 79, 'ś': 74, 'π': 288, ')': 165, 'я': 320, 'β': 301, 'τ': 302, '−': 303, '¸': 299, 'п': 305, '\\x92': 51, 'ê': 309, '루': 249, '+': 310, 'Ş': 159, '守': 306, 'У': 313, \"'\": 315, '3': 205, '¹': 322, '″': 318, '0': 319, 'ñ': 321, 'ň': 263, '≤': 112, 'û': 54, '·': 145, 'с': 169, '«': 192, 'ŏ': 329, 'č': 50, 'Č': 229, '4': 332, '九': 278, '太': 292}\n",
      "{'allUpper': 3, 'contains_digit': 7, 'numeric': 1, 'mainly_numeric': 6, 'other': 5, 'allLower': 2, 'initialUpper': 4, 'PADDING_TOKEN': 0}\n"
     ]
    }
   ],
   "source": [
    "print(models.idx2Label)\n",
    "print(models.label2Idx)\n",
    "print(models.char2Idx)\n",
    "print(models.case2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseEmbeddings = np.identity(len(models.case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.ft = fastText.load_model(\"../embeddings/wiki.de.bin\")\n",
    "\n",
    "nb_embedding_dims = ft.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBatches(dataset):\n",
    "    l = []\n",
    "    for i in dataset:\n",
    "        l.append(len(i))\n",
    "    l = set(l)\n",
    "    print(len(l))\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        temp = []\n",
    "        for batch in dataset:\n",
    "            if len(batch) == i:\n",
    "                temp.append(batch)\n",
    "                z += 1\n",
    "        batches.append(temp)\n",
    "#         batch_len.append(z)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "test_batches = createBatches(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batches: 'list of training/dev sentences- batches already created'):\n",
    "    global line_number\n",
    "    \n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            word_embeddings = []\n",
    "            case_embeddings = []\n",
    "            char_embeddings = []\n",
    "\n",
    "            output_labels = []\n",
    "            for index in range(len(batch)): # batches made according to the size of the sentences. len(batch) gives the size of current batch            \n",
    "                sentence = batch[index]\n",
    "                temp_casing = []\n",
    "                temp_char=[]\n",
    "                temp_word=[]\n",
    "                temp_output=[]\n",
    "                for word in sentence:\n",
    "                    word, label = word\n",
    "                    casing = utils.getCasing(word, models.case2Idx)\n",
    "                    temp_casing.append(casing)\n",
    "                    temp_char2=[]\n",
    "                    for char in word:\n",
    "                        if char in models.char2Idx.keys():\n",
    "                            temp_char2.append(models.char2Idx[char])\n",
    "                        else:\n",
    "                            temp_char2.append(models.char2Idx['UNKNOWN']) # To incorporate the words which are not in the vocab\n",
    "                    temp_char2 = np.array(temp_char2)\n",
    "                    temp_char.append(temp_char2)\n",
    "                    # word_vector = ft.get_word_vector(word.lower())\n",
    "                    word_vector = ft.get_word_vector(word)\n",
    "                    temp_word.append(word_vector)\n",
    "                    temp_output.append(models.label2Idx[label])\n",
    "                temp_char = pad_sequences(temp_char, 52)\n",
    "                word_embeddings.append(temp_word)\n",
    "                case_embeddings.append(temp_casing)\n",
    "                char_embeddings.append(temp_char)\n",
    "                temp_output = to_categorical(temp_output, len(models.idx2Label)+1)\n",
    "                output_labels.append(temp_output)\n",
    "            yield ([np.array(word_embeddings), np.array(case_embeddings), np.array(char_embeddings)], np.array(output_labels))\n",
    "\n",
    "def get_label_from_categorical(a):\n",
    "    labels = []\n",
    "    for label in a:\n",
    "        label = np.ndarray.tolist(label)\n",
    "        label = np.argmax(label)\n",
    "        labels.append(label)\n",
    "    return(labels)\n",
    "\n",
    "def predict_batches(batch):\n",
    "    steps = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for input_data, output_data in generator(batch):\n",
    "        pred_labels_batch = model.predict(input_data)\n",
    "        for s in pred_labels_batch:\n",
    "            pred_labels.append(get_label_from_categorical(s))\n",
    "        for s in output_data:\n",
    "            true_labels.append(get_label_from_categorical(s))\n",
    "        steps += 1\n",
    "        if steps == len(batch):\n",
    "            break\n",
    "    return(true_labels, pred_labels)\n",
    "\n",
    "tmp_model_filename = 'model_lstm_germeval_v2.0.h5'\n",
    "checkpoint = ModelCheckpoint(tmp_model_filename, verbose=1, save_best_only = True, monitor = 'val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    caseEmbeddings = np.identity(len(models.case2Idx), dtype='float32')\n",
    "    words_input = Input(shape=(None, nb_embedding_dims), dtype='float32', name='words_input')\n",
    "    casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "    casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False, name = 'case_embed')(casing_input)\n",
    "    character_input=Input(shape=(None,nb_char_embeddings,),name='char_input')\n",
    "    embed_char_out=TimeDistributed(Embedding(len(models.char2Idx),32,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "    char_lstm = TimeDistributed(Bidirectional(LSTM(50, name = 'char_lstm')))(embed_char_out)\n",
    "    output = concatenate([words_input, casing, char_lstm])\n",
    "    output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.5, name = 'token_lstm'))(output)\n",
    "    output = TimeDistributed(Dense(len(models.label2Idx), name = 'token_dense'))(output)\n",
    "    crf = CRF(len(models.label2Idx), name = 'crf')\n",
    "    output = crf(output)\n",
    "    model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "    model.compile(loss=crf.loss_function, optimizer='nadam', metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/root/miniconda3/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, 52)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 52, 32) 10688       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "case_embed (Embedding)          (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 100)    33200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, None, 408)    0           words_input[0][0]                \n",
      "                                                                 case_embed[0][0]                 \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, None, 400)    974400      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 26)     10426       bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "crf (CRF)                       (None, None, 26)     1430        time_distributed_10[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,030,208\n",
      "Trainable params: 1,030,144\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_char_embeddings = 52\n",
    "model = get_model()\n",
    "model.load_weights(tmp_model_filename)\n",
    "history = utils.F1History(tmp_model_filename, devSet = devSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "52/94 [===============>..............] - ETA: 2:35 - loss: -0.2800 - crf_viterbi_accuracy: 0.9943"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-babccc0f322a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNerSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNerSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevSentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    utils.NerSequence(trainSentences, shuffle_data=True, batch_size=256), \n",
    "    validation_data = utils.NerSequence(devSentences, batch_size=256), \n",
    "    epochs = 10, callbacks = [history]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Folge', 'O'], 'O')\n",
      "(['ausgestrahlt', 'O'], 'O')\n",
      "(['wird', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Ergativkonstruktionen', 'O'], 'O')\n",
      "(['sind', 'O'], 'O')\n",
      "(['häufig', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Auch', 'O'], 'O')\n",
      "(['hier', 'O'], 'O')\n",
      "(['Widerspruch', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Und', 'O'], 'O')\n",
      "(['Tischer', 'B-PER'], 'O')\n",
      "(['versichert', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Das', 'O'], 'O')\n",
      "(['ist', 'O'], 'O')\n",
      "(['nachprüfbar', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Nicht', 'O'], 'O')\n",
      "(['weggeschmissene', 'O'], 'O')\n",
      "(['Briefe', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Oder', 'O'], 'O')\n",
      "(['vier', 'O'], 'O')\n",
      "(['alle', 'O'], 'O')\n",
      "(['?', 'O'], 'O')\n",
      "\n",
      "(['Wir', 'O'], 'O')\n",
      "(['werden', 'O'], 'O')\n",
      "(['evakuiert', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Münchner', 'B-ORG'], 'B-LOCderiv')\n",
      "(['Neueste', 'I-ORG'], 'O')\n",
      "(['Nachrichten', 'I-ORG'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Sogar', 'O'], 'O')\n",
      "(['mit', 'O'], 'O')\n",
      "(['eigenem', 'O'], 'O')\n",
      "(['Skilift', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Das', 'O'], 'O')\n",
      "(['Ergebnis', 'O'], 'O')\n",
      "(['ist', 'O'], 'O')\n",
      "(['ernüchternd', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Auch', 'O'], 'O')\n",
      "(['Neuaufträge', 'O'], 'O')\n",
      "(['würden', 'O'], 'O')\n",
      "(['angenommen', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Die', 'O'], 'O')\n",
      "(['Füße', 'O'], 'O')\n",
      "(['waren', 'O'], 'O')\n",
      "(['graubraun', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Der', 'O'], 'O')\n",
      "(['Fisch', 'O'], 'B-OTH')\n",
      "(['ist', 'O'], 'O')\n",
      "(['gut', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Ist', 'O'], 'O')\n",
      "(['mal', 'O'], 'O')\n",
      "(['was', 'O'], 'O')\n",
      "(['anderes', 'O'], 'O')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(['Was', 'O'], 'B-ORG')\n",
      "(['hat', 'O'], 'O')\n",
      "(['Hearst', 'B-PER'], 'B-PER')\n",
      "(['kreiert', 'O'], 'O')\n",
      "(['?', 'O'], 'O')\n",
      "\n",
      "(['Das', 'O'], 'O')\n",
      "(['einmalige', 'O'], 'O')\n",
      "(['Dorf', 'O'], 'B-LOC')\n",
      "(['Oberrodenbach', 'B-LOC'], 'I-LOC')\n",
      "(['.', 'O'], 'O')\n",
      "\n",
      "(0.6137908708319845, 0.5971653543307086, 0.6053639846743295)\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels = predict_batches(test_batches)\n",
    "sentences = [sent for batch in test_batches for sent in batch]\n",
    "for i, (true_sentence, pred_sentence) in enumerate(zip(true_labels, pred_labels)):\n",
    "    for j, (true_word, pred_word) in enumerate(zip(true_sentence, pred_sentence)):\n",
    "        print((sentences[i][j], models.idx2Label[pred_word]))\n",
    "        \n",
    "    print()\n",
    "print(compute_f1(true_labels, pred_labels, models.idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
