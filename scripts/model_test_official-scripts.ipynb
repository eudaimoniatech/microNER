{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import fastText\n",
    "\n",
    "from validation import compute_f1\n",
    "import conlleval\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "import models\n",
    "import utils\n",
    "\n",
    "import shutil, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "def create_custom_objects():\n",
    "    instanceHolder = {\"instance\": None}\n",
    "    class ClassWrapper(CRF):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            instanceHolder[\"instance\"] = self\n",
    "            super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "    def loss(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"loss_function\")\n",
    "        return method(*args)\n",
    "    def accuracy(*args):\n",
    "        method = getattr(instanceHolder[\"instance\"], \"accuracy\")\n",
    "        return method(*args)\n",
    "    return {\"ClassWrapper\": ClassWrapper ,\"CRF\": ClassWrapper, \"loss\": loss, \"accuracy\":accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "testSentences = utils.get_sentences_germeval('../../Resources/GermEVAL/NER-de-test.tsv')\n",
    "print(len(testSentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.ft = fastText.load_model(\"../../fastText/wiki.de.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model_indexes(indexes_file):\n",
    "    indexMappings = json.load(open(indexes_file, \"r\"))\n",
    "    models.idx2Label = {int(k):v for k,v in indexMappings[0].items()}\n",
    "    models.label2Idx = indexMappings[1]\n",
    "    models.char2Idx = indexMappings[2]\n",
    "    models.case2Idx = indexMappings[3]\n",
    "    models.max_sequence_length = 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_indexes('model_lstm_germeval_v2.0.h5.indexes')\n",
    "finalmodel = load_model('model_lstm_germeval_v2.0.h5', custom_objects=create_custom_objects())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8325358851674641, 0.8167691809647135, 0.8245771713375276)\n"
     ]
    }
   ],
   "source": [
    "true_labels, pred_labels = utils.predict_sequences(finalmodel, testSentences)\n",
    "print(compute_f1(pred_labels, true_labels, models.idx2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official script eval CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conlleval\n",
    "\n",
    "eval_file = 'test_pl.tsv'\n",
    "write_conll_file(true_labels, pred_labels, testSentences, models.idx2Label, eval_file) \n",
    "p,r,f = evaluate_conll_file(eval_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(p)\n",
    "print(r)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_base_name = 'model_lstm_germeval_v2.'\n",
    "eval_file = 'test_pl.tsv'\n",
    "all_f1_scores = []\n",
    "\n",
    "for run_i in range(10):\n",
    "    model_file = file_base_name + str(run_i) + '.h5'\n",
    "    index_file = file_base_name + str(run_i) + '.h5.indexes'\n",
    "    if not os.path.isfile(model_file):\n",
    "        print(model_file + ' not found')\n",
    "        continue\n",
    "    if not os.path.isfile(index_file):\n",
    "        print(model_file + ' not found')\n",
    "        continue\n",
    "    \n",
    "    print('Evaluating model ' + model_file)\n",
    "    \n",
    "    load_model_indexes(index_file)\n",
    "    finalmodel = load_model(model_file, custom_objects=create_custom_objects())\n",
    "    \n",
    "    true_labels, pred_labels = utils.predict_sequences(finalmodel, testSentences)\n",
    "    \n",
    "    conlleval.write_conll_file(true_labels, pred_labels, testSentences, models.idx2Label, eval_file) \n",
    "    p,r,f = conlleval.evaluate_conll_file(eval_file) \n",
    "    all_f1_scores.append(f)\n",
    "\n",
    "print(all_f1_scores)\n",
    "print(np.mean(all_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(conlleval)\n",
    "# importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official script eval Germeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1st = models.idx2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ORGderiv': 2, 'B-PER': 10, 'PADDING_TOKEN': 0, 'I-ORGpart': 5, 'I-LOCpart': 17, 'B-OTHpart': 24, 'I-OTH': 4, 'B-ORG': 18, 'I-LOCderiv': 22, 'B-PERderiv': 11, 'B-LOC': 12, 'I-OTHderiv': 20, 'I-ORG': 15, 'B-OTH': 25, 'I-PER': 19, 'B-PERpart': 23, 'I-ORGderiv': 3, 'I-PERpart': 13, 'I-OTHpart': 7, 'O': 9, 'B-ORGpart': 14, 'I-PERderiv': 8, 'B-LOCderiv': 21, 'I-LOC': 1, 'B-LOCpart': 16, 'B-OTHderiv': 6}\n",
      "{0: 'PADDING_TOKEN', 1: 'I-LOC', 2: 'B-ORGderiv', 3: 'I-ORGderiv', 4: 'I-OTH', 5: 'I-ORGpart', 6: 'B-OTHderiv', 7: 'I-OTHpart', 8: 'I-PERderiv', 9: 'O', 10: 'B-PER', 11: 'B-PERderiv', 12: 'B-LOC', 13: 'I-PERpart', 14: 'B-ORGpart', 15: 'I-ORG', 16: 'B-LOCpart', 17: 'I-LOCpart', 18: 'B-ORG', 19: 'I-PER', 20: 'I-OTHderiv', 21: 'B-LOCderiv', 22: 'I-LOCderiv', 23: 'B-PERpart', 24: 'B-OTHpart', 25: 'B-OTH'}\n"
     ]
    }
   ],
   "source": [
    "print(models.label2Idx)\n",
    "print(idx_1st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5807127882599581, 0.537864077669903, 0.5584677419354839)\n"
     ]
    }
   ],
   "source": [
    "testSentences_2nd = utils.get_sentences_germeval('../../Resources/GermEVAL/NER-de-test.tsv', level2=True)\n",
    "load_model_indexes('model_lstm_germeval_2nd-level.h5.indexes')\n",
    "finalmodel_2nd = load_model('model_lstm_germeval_2nd-level.h5', custom_objects=create_custom_objects())\n",
    "true_labels_2nd, pred_labels_2nd = utils.predict_sequences(finalmodel_2nd, testSentences_2nd)\n",
    "print(compute_f1(pred_labels_2nd, true_labels_2nd, models.idx2Label))\n",
    "idx_2nd = models.idx2Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = 'germeval_outer_inner.tsv'\n",
    "conlleval.write_germeval_file(true_labels, pred_labels, true_labels_2nd, pred_labels_2nd, testSentences, idx_1st, idx_2nd, eval_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
